33498	ganesh_baliga sanjay_jain arun_sharma	learning from multiple source of inaccurate data	most theoretical model of inductive inference make the idealized assumption that the datum available to a learner be from a single and accurate source the subject of inaccuracy in datum emanate from a single source have be address by several author the present paper argue in favor of a more realistic learning model in which datum emanate from multiple source some or all of which may be inaccurate three kind of inaccuracy be consider spurious datum lrb model as noisy text rrb miss datum lrb model as incomplete text rrb and a mixture of spurious and missing datum lrb model as imperfect text rrb motivate by the above argument the present paper introduce and theoretically analyze a number of inference criterion in which a learn machine be feed datum from multiple source some of which may be infect with inaccuracy the learning situation model be the identification in the limit of program from graph of computable function the main parameter of the investigation be kind of inaccuracy total number of datum source number of faulty datum source which produce datum within a acceptable bind and the bind on the number of error allow in the final hypothesis learn by the machine sufficient condition be determine under which for the same kind of inaccuracy for the same bind on the number of error in the final hypothesis and for the same bind on the number of inaccuracy learn from multiple text some of which may be inaccurate be equivalent to learn from a single inaccurate text the general problem of determine when learn from multiple inaccurate text be a restriction over learn from a single inaccurate text turn out to be combinatorially very complex significant partial result be provide for this problem several result be also provide about condition under which the detrimental effect of multiple text can be overcome by either allow more error in the final hypothesis or by reduce the number of inaccuracy in the text it be also show that the usual hierarchy result from allow extra error in the final program lrb result in increase learn power rrb and allow extra inaccuracy in the text lrb result in decrease learning power rrb hold finally it be demonstrate that in the context of learn from multiple inaccurate text spurious datum be better than miss datum which in turn be better than a mixture of spurious and missing datum 1 introduction a scenario in which a algorithmic learner doi 101007 35405600418 multiple text learning power inaccuracy spurious multiple source	AII	
33512	john_case keh-jiann_chen sanjay_jain	strong separation of learning class	suppose lc 1 and lc 2 be two machine learning class each base on a criterion of success suppose for every machine which learn a class of function accord to the lc 1 criterion of success there be a machine which learn this class accord to the lc 2 criterion in the case where the converse do not hold lc 1 be say to be separate from lc 2 it be show that for many such separated learning class from the literature a much stronger separation hold lrb c lc 1 rrb lrb c lrb lc 2 lc 1 rrb rrb lsb c c rsb it be also show that there be a pair of separated learning class from the literature for which the stronger separation just above do not hold a philosophical heuristic toward the design of artificially intelligent learning program be present with each strong separation result doi 101007 35405600419 correspond interpretation technical result est input and output criterion of success	AII	
39179	mark_a._fulk sanjay_jain	approximate inference and scientific method	a new identification criterion motivate by notion of successively improve approximation in the philosophy of science be define it be show that the class of recursive function be identifiable under this criterion this result be extend to permit somewhat more realistic type of datum than usual this criterion be then modify to consider restriction on the quality of approximation and the new criterion be compare to exist criterion doi 101006 inco 19941084	ALT	
39193	lorenzo_carlucci john_case sanjay_jain frank_stephan	non ushaped vacillatory and team learning	ushaped learning behaviour in cognitive development involve learning unlearn and relearn it occur for example in learn irregular verb the prior cognitive science literature be occupy with how human do it for example general rule versus table of exception this paper be mostly concern with whether ushaped learning behaviour may be necessary in the abstract mathematical setting of inductive inference that be in the computational learning theory follow the framework of gold all notion consider be learn from text that be from positive datum previous work show that ushaped learning behaviour be necessary for behaviourally correct learning but not for syntactically convergent learn in the limit lrb explanatory learning rrb the present paper establish the necessity for the hierarchy of class of vacillatory learning where a behaviourally correct learner have to satisfy the additional constraint that it vacillate in the limit between at most b grammar where b lcb 2 3 rcb non ushaped vacillatory learning be show to be restrictive every non ushaped vacillatorily learnable class be already learnable in the limit furthermore if vacillatory learning with the parameter b 2 be possible then non ushaped behaviourally correct learning be also possible but for b 3 surprisingly there be a class witness that this implication fail doi 101007 1156408920 learning behaviour txtex language l txtbc grammar	ALT	
39201	john_case sanjay_jain	synthesize learners tolerating computable noisy data	a index for a re class of language lrb by deenition rrb generate a sequence of grammar deen the class a index for a index family of language lrb by deenition rrb generate a sequence of decision procedure deen the family f stephan s model of noisy datum be employ in which roughly correct datum crop up innnitely often and incorrect datum only nitely often in a completely computable universe all datum sequence even noisy one be computable new to the present paper be the restriction that noisy datum sequence be nonetheless computable study then be the synthesis from index for re class and for index family of language of various kind of noisetolerant languagelearner for the corresponding class or family index where the noisy input datum sequence be restricted to be computable many positive result as well as some negative result be present regard the existence of such synthesizer the main positive result be surprisingly more positive than its analog in the case the noisy datum be not restricted to be computable grammar for each index family can be learn behaviorally correctly from computable noisy positive datum the proof of another positive synthesis result yield as a pleasant corollary a strict subsetprinciple or telltale style characterization for the computable noisetolerant behaviorally correct learnability of grammar from positive and negative datum of the corresponding family index doi 101007 354049730716 decision procedure deenition noisy data positive datum grammar	ALT	
39203	john_case sanjay_jain susanne_kaufmann arun_sharma frank_stephan	predictive learning model for concept drift	concept drift mean that the concept about which data be obtain may shift from time to time each time after some minimum permanence except for this minimum permanence the concept shift may not have to satisfy any further requirement and may occur infinitely often within this work be study to what extent it be still possible to predict or learn value for a data sequence produce by drift concept various way to measure the quality of such prediction include martingale bet strategy and density and frequency of correctness be introduce and compare with one another for each of these measure of prediction quality for some interesting concrete class lrb nearly rrb optimal bound on permanence for attain learnability be establish the concrete class from which the drift concept be select include regular language accept by finite automata of bound size polynomial of bound degree and sequence define by recurrence relation of bound size some important restricted case of drift be also study for example the case where the interval of permanence be computable in the case where the concept shift only 1 among finitely many possibility from certain infinite arguably practical class the learn algorithm can be considerably improve doi 101007 354049730721 drift martingale permanence nse iff	ALT	
39205	john_case sanjay_jain rudiger_reischuk frank_stephan thomas_zeugmann	learn a subclass of regular patterns in polynomial time	present be a algorithm lrb for learn a subclass of erase regular pattern language rrb which can be make to run with arbitrarily high probability of success on extended regular language generate by pattern of the form x01x1 mxm for unknown m but known c from number of example polynomial in m lrb and exponential in c rrb where x0 xm be variable and where 1 m be each string of constant or terminal of length c this assume that the algorithm randomly draw sample with natural and plausible assumption on the distribution the more general look case of extended regular pattern which alternate between a variable and fixed length constant string begin and end with either a variable or a constant string be similarly handle doi 101016 jtcs 200607044 subclass regular pattern length c constant string	ALT	
39213	john_case sanjay_jain arun_sharma	anomalous learning help succinctness lrb extended abstract rrb		ALT	
39215	john_case sanjay_jain arun_sharma	machine induction without revolutionary paradigm shift		ALT	
39217	john_case sanjay_jain frank_stephan	vacillatory and bc learning on noisy data	in a earlier paper frank stephan introduce a form of noisy datum which nonetheless uniquely determine the true datum correct information occur innnitely often while incorrect information occur only nitely often the present paper consider the eeect of this form of noise on vacillatory and behaviorally correct learning of grammar both from positive datum alone and from informant lrb positive and negative datum rrb for learn from informant the noise in eeect destroy negative datum various noisydata hierarchy be exhibit which in some case be know to collapse when there be no noise noisy behaviorally correct learning be show to obey a very strong subset principle it be show in many case how much power be need to overcome the eeect of noise for example the best we can do to simulate in the presence of noise the noisefree no mind change case take innnitely many mind change one technical result be prove by a priority argument doi 101007 354061863553 eeect informant negative datum correct information	ALT	
39219	john_case sanjay_jain arun_sharma	synthesize noisetolerant language learner	a index for a re class of language lrb by deenition rrb generate a sequence of grammar deen the class a index for a index family of language lrb by deenition rrb generate a sequence of decision procedure deen the family f stephan s model of noisy datum be employ in which roughly correct datum crop up innnitely often and incorrect datum only nitely often study then be the synthesis from index for re class and for index family of language of various kind of noisetolerant languagelearner for the corresponding class or family index many positive result as well as some negative result be present regard the existence of such synthesizer the proof of most of the positive result yield as pleasant corollary strict subsetprinciple or telltale style characterization for the noisetolerant learnability of the corresponding class or family index doi 101016 s03043975 lrb 00 rrb 001328 deenition positive result re class	ALT	
39369	sanjay_jain	program synthesis in the presence of infinite number of inaccuracies	most study modeling inaccurate datum in gold style learn consider case in which the number of inaccuracy be finite the present paper argue that this approach be not reasonable for modeling inaccuracy in concept that be infinite in nature lrb for example graph of computable function rrb the effect of infinite number of inaccuracy in the input datum in gold s model of learning be consider in the context of identification in the limit of computer program from graph of computable function three kind of inaccuracy namely noisy datum incomplete datum and imperfect datum be consider the amount of each of these inaccuracy in the input be measure use certain density notion a number of interesting hierarchy result be show base on the density of inaccuracy present in the input datum several result establish tradeoff between the density and type of inaccuracy be also derive doi 101007 354058520675 computer program inaccuracy computable function finite number infinite number	AII/ALT	
39370	sanjay_jain	learn with refutation	in they pioneering work mukouchi and arikawa model a learning situation in which the learner be expect to refute text which be not representative of l the class of language be identify lange and watson extend this model to consider justified refutation in which the learner be expect to refute text only if it contain a finite sample unrepresentative of the class l both the above study be in the context of index family of recursive language we extend this study in two direction firstly we consider general class of recursively enumerable language secondly we allow the machine to either identify or refute the unrepresentative text lrb respectively text contain finite unrepresentative sample rrb we observe some surprising difference between we result and the result obtain for learn index family by lange and watson doi 101006 jcss 19981591 txtex class of languages input text refutation general class	ALT	
39371	sanjay_jain efim_b._kinber	learning languages from positive datum and negative counterexample	in this paper we introduce a paradigm for learn in the limit of potentially infinite language from all positive datum and negative counterexample provide in response to the conjecture make by the learner several variant of this paradigm be consider that reflect different conditionsconstraints on the type and size of negative counterexample and on the time for obtain they in particular we consider the model where 1 rrb a learner get the least negative counterexample 2 rrb the size of a negative counterexample must be bound by the size of the positive datum see so far 3 rrb a counterexample can be delay learn power limitation of these model relationship between they as well as they relationship with classical paradigm for learn language in the limit lrb without negative counterexample rrb be explore several surprising result be obtain in particular for gold s model of learn require a learner to syntactically stabilize on correct conjecture learner get negative counterexample immediately turn out to be as powerful as the one that do not get they for indefinitely lrb but finitely rrb long time lrb or be only tell that they latest conjecture be not a subset of the target language without any specific negative counterexample rrb another result show that for behaviourally correct learning lrb where semantic convergence be require from a learner rrb with negative counterexample a learner make just one error in almost all its conjecture have the ultimate power it can learn the class of all recursively enumerable language yet another result demonstrate that sometimes positive datum and negative counterexample provide by a teacher be not enough to compensate for full positive and negative datum doi 101016 jjcss 200706012 input language negative counterexample positive datum informant grammar	ALT	
39372	sanjay_jain efim_b._kinber	learning multiple languages in group	we consider a variant of gold s learning paradigm where a learner receive as input n different language lrb in form of one text where all input language be interleave rrb we goal be to explore the situation when a more coarse classification of input language be possible whereas more refined classification be not more specifically we answer the follow question under which condition a learner be feed n different language can produce m grammar cover all input language but can not produce k grammar cover input language for any k m we also consider a variant of this task where each of the output grammar may not cover more than r input language we main result indicate that the major factor affect classification capability be the difference n m between the number n of input language and the number m of output grammar we also explore relationship between classification capability for smaller and larger group of input language for the variant of we model with the upper bind on the number of language allow to be represent by one output grammar for class consist of disjoint language we find complete picture of relationship between classification capability for different parameter n lrb the number of input language rrb m lrb number of output grammar rrb and r lrb bind on the number of language represent by each output grammar rrb this picture include a combinatorial characterization of classification capability for the parameter n m r of certain type doi 101016 jtcs 200707025 corollary input language n language line of research classification capability	ALT	
39373	sanjay_jain efim_b._kinber	learning and extending sublanguages	a number of natural model for learn in the limit be introduce to deal with the situation when a learner be require to provide a grammar cover the input even if only a part of the target language be available example of language family be exhibit that be learnable in one model and not learnable in another one some characterization for learnability of algorithmically enumerable family of language for the model in question be obtain since learnability of any part of the target language do not imply monotonicity of the learning process we consider also we model under additional monotonicity constraint doi 101007 1189484114 learnability sublanguage monotonicity target language grammar	ALT	
39374	sanjay_jain efim_b._kinber	iterative learning from positive datum and negative counterexample	a model for learn in the limit be define where a lrb socalled iterative rrb learner get all positive example from the target language test every new conjecture with a teacher lrb oracle rrb if it be a subset of the target language lrb and if it be not then it receive a negative counterexample rrb and use only limited longterm memory lrb incorporate in conjecture rrb three variant of this model be compare when a learner receive least negative counterexample the one whose size be bound by the maximum size of input see so far and arbitrary one a surprising result be that sometimes absence of bounded counterexample can help a iterative learner whereas arbitrary counterexample be useless we also compare we learnability model with other relevant model of learnability in the limit study how we model work for index class of recursive language and show that learner in we model can work in nonushaped way never abandon the first right conjecture doi 101007 1189484115 negative datum longterm memory ncit learnability counterexample	ALT	
39375	sanjay_jain efim_b._kinber rolf_wiehagen	on learning and colearning of minimal program		ALT	
39376	sanjay_jain efim_b._kinber rolf_wiehagen thomas_zeugmann	learning recursive functions refutably	learning of recursive function refutably mean that for every recursive function the learn machine have either to learn this function or to refute it ie to signal that it be not able to learn it three modi of make precise the notion of refute be consider we show that the corresponding type of learn refutably be of strictly increase power where already the most stringent of they turn out to be of remarkable topological and algorithmical richness all these type be close under union though in different strength also these type be show to be different with respect to they intrinsic complexity two of they do not contain function class that be most difficult to learn while the third one do moreover we present characterization for these type of learn refutably some of these characterization make clear where the refute ability of the corresponding learning machine come from and how it can be realize in general for learn with anomaly refutably we show that several result from standard learning without refutation stand refutably then we derive hierarchy for refutable learning finally we show that stricter refutability constraint can not be trade for more liberal learning criterion doi 101007 354045583322 rel refuting learning machine relex	ALT	
39377	sanjay_jain steffen_lange jochen_nessel	learning of re languages from good example		ALT	
39378	sanjay_jain steffen_lange sandra_zilles	goldstyle and query learning under various constraint on the target class	in language learning strong relationship between goldstyle model and query model have recently be observe in some quite general setting goldstyle learner can be replace by query learner and vice versa without loss of learn capability these equality hold in the context of learn indexable class of recursive language former study on goldstyle learning of such indexable class have show that in many setting the enumerability of the target class and the recursiveness of its language be crucial for learnability moreover study query learning nonindexable class have be mainly neglect up to now so it be conceivable that the recently observe relation between goldstyle and query learning be not due to common structure in the learn process in both model but rather to the enumerability of the target class or the recursiveness of they language in this paper the analysis be lift onto the context of learn arbitrary class of re language still strong relationship between the approach of goldstyle and query learning be prove but there be significant change to the former result though in many case learner of one type can still be replace by learner of the other type in general this do not remain valid vice versa all result hold even for learn class of recursive language which indicate that the recursiveness of the language be not crucial for the former equality result thus we analyse how constraint on the algorithmic structure of the target class affect the relation between two approach to language learning doi 101007 1156408919 target concept recursiveness query learning indexable class strong relationship	ALT	
39379	sanjay_jain steffen_lange sandra_zilles	towards a better understand of incremental learning	the present study aim at insight into the nature of incremental learning in the context of gold s model of identification in the limit with a focus on natural requirement such as consistency and conservativeness incremental learning be analyse both for learn from positive example and for learn from positive and negative example the result obtain illustrate in which way different consistency and conservativeness demand can affect the capability of incremental learner these result may serve as a first step towards characterise the structure of typical class learnable incrementally and thus towards elaborate uniform incremental learning method doi 101007 1189484116 conservativeness target concept incremental learning positive example iterative learning	ALT	
39380	sanjay_jain wolfram_menzel frank_stephan	class with easily learnable subclass	in this paper we study the question of whether identifiable class have subclass which be identifiable under a more restrictive criterion the choose framework be inductive inference in particular the criterion of explanatory learning lrb ex rrb of recursive function as introduce by gold in 1967 among the more restrictive criterion be finite learning where the learner output on every function to be learn exactly one hypothesis lrb which have to be correct rrb the topic of the present paper be the natural variant lrb a rrb and lrb b rrb below of the classical question whether a give learning criterion like finite learning be more restrictive than exlearning lrb a rrb do every infinite exidentifiable class have a infinite finitely identifiable subclass lrb b rrb if a infinite exidentifiable class s have a infinite finitely identifiable subclass do it necessarily follow that some appropriate learner exidentify s as well as finitely identify a infinite subclass of s these question be also treat in the context of ordinal mind change bound doi 101007 354036169319 finite learning infinite mind change subclasses class s	ALT	
39381	sanjay_jain eric_martin frank_stephan	absolute versus probabilistic classification in a logical setting	suppose we be give a set w of logical structure or possible world a set of logical formula call possible datum and a logical formula we then consider the classification problem of determine in the limit and almost always correctly whether a possible world m satisfy from a complete enumeration of the possible datum that be true in m one interpretation of almost always correctly be that the classification might be wrong on a set of possible world of measure 0 with respect to some natural probability distribution over the set of possible world another interpretation be that the classifier be only require to classify a set w of possible world of measure 1 without have to produce any claim in the limit on the truth of for the member of the complement of w in w we compare these notion with absolute classification of w with respect to a formula that be almost always equivalent to in w hence investigate whether the set of possible world on which the classification be correct be definable we mainly work with the probability distribution that correspond to the standard measure on the cantor space but we also consider a alternative probability distribution propose by solomonoff and contrast it with the former finally in the spirit of the kind of computation consider in logic programming we address the issue of compute almost correctly in the limit witness to lead existentially quantify variable in existential formula doi 101007 1156408926 logical formula successor possible world measure 1 possible datum	ALT	
39382	sanjay_jain yen_kaow_ng tiong_seng_tay	learning languages in a union	in inductive inference a machine be give word of a language lrb a recursively enumerable set in we setting rrb and the machine be say to identify the language if it correctly name the language in this paper we study identifiability of class of language where the union of up to a fixed number lrb n say rrb of language from the class be provide as input we distinguish between two different scenario in one scenario the learner need only to name the language which result from the union in the other the learner must individually name the language which make up the union lrb we say that the unioned language be discerningly identify rrb we define three kind of identification criterion base on this and by the use of some class of disjoint language demonstrate that the infer power of each of these identification criterion decrease as we increase the number of language allow in the union thus result in a infinite hierarchy for each identification criterion that be we show that for each n there exist a class of disjoint language where all union of up to n language from this class can be discerningly identify but there be no learner which identify every union of n 1 language from this class a comparison between the different identification criterion also yield similar hierarchy we give sufficient condition for class of language where the union can be discerningly identify and characterize such discerning learnability for the index family we then give naturally occur class of language that witness some of the earlier hierarchical result finally we present language class which be complete with respect to weak reduction lrb in term of intrinsic complexity rrb for we identification criterion doi 101016 jjcss 200601005 n language learnability identifiability discerning identification criterion	ALT	
39383	sanjay_jain frank_stephan	learning by switch type of information	the present work be dedicate to the study of mode of datapresentation in the range between text and informant within the framework of inductive inference in this study the learner alternatingly request sequence of positive and negative datum we define various formalization of valid datum presentation in such a scenario we resolve the relationship between these different formalization and show that one of these be equivalent to learn from informant we also show a hierarchy form lrb for each of the formalization study rrb by consider the number of switch between request for positive and negative datum doi 101007 354045583317 negative datum formalization learnable iff informant	ALT	
39384	sanjay_jain frank_stephan	learning how to separate	braincomputer interface lrb bci rrb extract signal from neural activity to control remote device range from computer cursor to limblike robot they show great potential to help patient with severe motor deficit perform everyday task without the constant assistance of caregiver understand the neural mechanism by which subject use bci system could lead to improve design and provide unique insight into normal motor control and skill acquisition however report vary considerably about how much training be require to use a bci system the degree to which performance improve with practice and the underlie neural mechanism this review examine these diverse finding they potential relationship with motor learning during overt arm movement and other outstanding question concern the volitional control of bci system doi 101016 jtins 201011003 arm movement control signal neural mechanism bci bmi	ALT	
39385	sanjay_jain arun_sharma	prudence in vacillatory language identification lrb extended abstract rrb		ALT	
39386	sanjay_jain arun_sharma	on aggregate team of learning machine	1 the present paper study the problem of when a team of learn machine can be aggregate into a single learning machine without any loss in learn power the main result concern aggregation ratio for vacillatory identiication of language from text for a positive i n teger n a m a c hine be say to txtfex nidentify a language l just in case the machine converge to up to n grammar for l on any text for l f or such identiication criterion the aggregation ratio be derive for the n 2 case it be show that the collection of language that can be txtfex 2 identii by team with success ratio greater than 5 6 be the same as those collection of language that can be txtfex 2identiied by a single machine it be also establish that 5 6 be indeed the cutoo point by show that there be collection of language that can be txtfex 2identiied by a team employ 6 machine at least 5 of which be require to be successful but can not be txtfex 2identiied by a n y single machine additionally aggregation ratio be also derive for nite identiication of language from positive datum and for numerous criterion involve language learn from both positive and negative datum doi 101016 03043975 lrb 94 rrb 00162c identiication txtex success ratio single machine grammar	ALT	
39387	sanjay_jain arun_sharma	on monotonic strategy for learning re languages		AII/ALT	
39388	sanjay_jain hans-ulrich_simon etsuji_tomita	editor introduction		ALT	
39666	sanjay_jain efim_b._kinber	oneshot learners use negative counterexample and nearest positive example	as some cognitive research suggest in the process of learn language in addition to overt explicit negative evidence a child often receive covert explicit evidence in form of correct or rephrase sentence in this paper we suggest one approach to formalization of overt and covert evidence within the framework of oneshot learner via subset and membership query to a teacher lrb oracle rrb we compare and explore general capability of we model as well as complexity advantage of learnability model of one type over model of other type where complexity be measure in term of number of query in particular we establish that correct positive example be sometimes more helpful to a learner than just negative lrb counter rrb example and access to full positive datum doi 101007 978354075225722 membership query finite positive example number of query counterexample	ALT	School_of_Computing National_University_of_Singapore 117590 Singapore
39667	sanjay_jain frank_stephan	learning in friedberg numberings	in this paper we consider learnability in some special numbering such as friedberg numbering which contain all the recursively enumerable language but have simpler grammar equivalence problem compare to acceptable numbering we show that every explanatorily learnable class can be learn in some friedberg numbering however such a result do not hold for behaviourally correct learning or finite learning one can also show that some friedberg numbering be so restrictive that all class which can be explanatorily learn in such friedberg numbering have only finitely many infinite language we also study similar question for several property of learner such as consistency conservativeness prudence iterativeness and non ushaped learning besides friedberg numbering we also consider the above problem for programming system with krecursive grammar equivalence problem doi 101016 jic 200803001 txtex learnability friedberg numbering learnable class grammar	ALT	Department_of_Computer_Science National_University_of_Singapore Singapore_117590 Republic_of_Singapore
39683	sanjay_jain frank_stephan ye_nan	prescribe learning of re class	this work extend study of angluin lange and zeugmann on the dependence of learn on the hypothesis space choose for the class in subsequent investigation uniformly recursively enumerable hypothesis space have be consider in the present work the follow four type of learn be distinguish classcomprising lrb where the learner can choose a uniformly recursively enumerable superclass as hypothesis space rrb classpreserving lrb where the learner have to choose a uniformly recursively enumerable hypothesis space of the same class rrb prescribe lrb where there must be a learner for every uniformly recursively enumerable hypothesis space of the same class rrb and uniform lrb like prescribe but the learner have to be synthesize effectively from a index of the hypothesis space rrb while for explanatory learning these four type of learnability coincide some or all be different for other learn criterion for example for conservative learning all four type be different several result be obtain for vacillatory and behaviourally correct learning three of the four type can be separate however the relation between prescribed and uniform learning remain open it be also show that every lrb not necessarily uniformly recursively enumerable rrb behaviourally correct learnable class have a prudent learner that be a learner use a hypothesis space such that it learn every set in the hypothesis space moreover the prudent learner can be effectively build from any learner for the class doi 101016 jtcs 200901011 uniform learnability hypotheses space iff same class	ALT	
39703	leonor_becerra-bonache john_case sanjay_jain frank_stephan	iterative learning of simple external contextual language	it be investigate for which choice of a parameter q denote the number of context the class of simple external contextual language be iteratively learnable on the one hand the class admit for all value of q polynomial time learnability provide a adequate choice of the hypothesis space be give on the other hand additional constraint like consistency and conservativeness or the use of a oneone hypothesis space change the picture iterative learning limit the long term memory of the learner to the current hypothesis and these constraint further hinder storage of information via padding of this hypothesis it be show that if q 3 then simple external contextual language be not iteratively learnable use a class preserving oneone hypothesis space while for q 1 it be iteratively learnable even in polynomial time it be also investigate for which choice of the parameter the simple external contextual language can be learn by a consistent and conservative iterative learner doi 101016 jtcs 201004009 hypothesis space learnable grammar mc	ALT	Department_of_Computer_Science National_University_of_Singapore Singapore Republic_of_Singapore_117590
39713	sanjay_jain frank_stephan	numberings optimal for learning	this paper extend previous study on learnability in nonacceptable numbering by consider the question for which criterion which numbering be optimal that be for which numbering it hold that one can learn every learnable class use the give numbering as hypothesis space furthermore a effective version of optimality be study as well it be show that the effectively optimal numbering for finite learning be just the acceptable numbering in contrast to this there be nonacceptable numbering which be optimal for finite learning and effectively optimal for explanatory vacillatory and behaviourally correct learning the numbering effectively optimal for explanatory learning be the kacceptable numbering a similar characterization be obtain for the numbering which be effectively optimal for vacillatory learning furthermore it be study which numbering be optimal for one and not for another criterion among the criterion of finite explanatory vacillatory and behaviourally correct learn all separation can be obtain however every numbering which be optimal for explanatory learning be also optimal for consistent learning doi 101016 jjcss 200908001 hypothesis space number learner m learnability iff	ALT	Department_of_Computer_Science National_University_of_Singapore Singapore Republic_of_Singapore_117543
94962	sanjay_jain eric_martin frank_stephan	inputdependence in functionlearning	in the standard model of inductive inference a learner get as input the graph of a function and have to discover lrb in the limit rrb a program for the function in this paper we consider besides the graph also other mode of input such as the complement of the graph the undergraph and the overgraph of the function the relationship between these model be study and a complete picture be obtain furthermore these notion be also explore for learn with oracle learn in team and learn in the presence of additional information doi 101007 s002240099174x finite set universe bridge positive datum oracle	CiE	Department_of_Computer_Science National_University_of_Singapore Singapore_117543 Republic_of_Singapore
108618	ganesh_baliga john_case sanjay_jain	synthesize enumeration technique for language learning	this paper provide positive and negative result on algorithmically synthesize from grammar and from decision procedure for class of language learn machine for identify from positive datum grammar for the language in those class in the process the uniformly decidable class of recursive language that can be behaviorally correctly identii from positive datum be surprisingly characterize by angluin s 1980 condition 2 lrb the subset principle for prevent overgeneralization rrb doi 101145 238061238090 grammar positive datum	COLT	
108778	lorenzo_carlucci john_case sanjay_jain frank_stephan	memorylimited ushaped learning	foreword this technical report contain a research paper development or tutorial article which have be submit for publication in a journal or for consideration by the commission organization the report represent the idea of its author and should not be take as the official view of the school or the university any discussion of the content of the report should be send to the author at the address show on the cover abstract ushaped learning be a learn behaviour in which the learner first learn something then unlearn it and finally relearn it such a behaviour observe by psychologist for example in the learning of pasttense of english verb have be widely discuss among psychologist and cognitive scientist as a fundamental example of the nonmonotonicity of learning previous theory literature have study whether or not ushaped learning in the context of gold s formal model of learn language from positive datum be necessary for learn some task it be clear that human learning involve memory limitation in the present paper we consider then this question of the necessity of ushaped learning for some learning model feature memory limitation we result show that the question of the necessity of ushaped learning in this memorylimited setting depend on delicate tradeoff between the learner s ability to remember its own previous conjecture to store some value in its longterm memory to make query about whether or not item occur in previously see datum and on the learner s choice of hypothesis space doi 101007 1177642020	COLT	
108779	lorenzo_carlucci sanjay_jain efim_b._kinber frank_stephan	variation on ushaped learning	foreword this technical report contain a research paper development or tutorial article which have be submit for publication in a journal or for consideration by the commission organization the report represent the idea of its author and should not be take as the official view of the school or the university any discussion of the content of the report should be send to the author at the address show on the cover abstract the paper deal with the follow problem be return to wrong conjecture necessary to achieve full power of learning return to wrong conjecture complement the paradigm of ushaped learning lsb 3792327 rsb when a learner return to old correct conjecture we explore we problem for classical model of learn in the limit txtexlearning when a learner stabilize on a correct conjecture and txtbclearning when a learner stabilize on a sequence of grammar represent the target concept in both case we show that surprisingly return to wrong conjecture be necessary to achieve full power of learning on the other hand it be neither necessary to resort to a invertedushaped behaviour lrb a wrongcorrectwrong pattern rrb nor to return to old overgeneralize conjecture contain element not belong to the target language we also consider we problem in the context of socalled vacillatory learning when a learner stabilize to a finite number of correct grammar in this case we show that return to old wrong conjecture use a invertedushaped strategy as well as return to old overgeneralize conjecture be necessary for full learn power we also show that surprisingly learner consistent with the input see so far can be make decisive lsb 324 rsb they do not have to return to any old conjecture wrong or right	COLT	
108781	john_case keh-jiann_chen sanjay_jain wolfgang_merkle james_s._royer	generality s price inescapable deficiency in machinelearned programs	this paper investigate some delicate tradeoff between the generality of a algorithmic learning device and the quality of the program it learn successfully there be result to the effect that thanks to small increase in generality of a learning device the computational complexity of some successfully learn program be provably unalterably suboptimal there be also result in which the complexity of successfully learn program be asymptotically optimal and the learn device be general but still thanks to the generality some of those optimal learn program be provably unalterably information deficient in some case deficient as to safe algorithmic extractabilityprovability of the fact that they be even approximately optimal for these result the safe algorithmic method of information extraction will be by proof in arbitrary true computably axiomatizable extension of peano arithmetic doi 101016 japal 200506013 learning device	COLT	
108782	john_case sanjay_jain franco_montagna giulia_simi andrea_sorbi	on learning to coordinate random bits help insightful normal form and competency isomorphism	a mere bounded number of random bit judiciously employ by a probabilistically correct algorithmic coordinator be show to increase the power of learn to coordinate compare to deterministic algorithmic coordinator furthermore these probabilistic algorithmic coordinator be provably not characterize in power by team of deterministic one a insightful enumeration technique base normal form characterization of the class that be learnable by total computable coordinator be give these normal form be for insight only since it be show that the complexity of the normal form of a total computable coordinator can be infeasible compare to the original coordinator montagna and osherson show that the competence class of a total coordinator can not be strictly improve by another total coordinator it be show in the present paper that the competency of any two total coordinator be the same modulo isomorphism furthermore a completely effective index set version of this competency isomorphism result be give where all the coordinator be total computable we also investigate the competence class of total coordinator from the point of view of topology and descriptive set theory doi 101007 978354045167951 coordinator normal form	COLT	
108783	john_case sanjay_jain matthias_ott arun_sharma frank_stephan	robust learning aid by context	empirical study of multitask learning provide some evidence that the performance of a learning system on its intended target improve by present to the learning system related task also call context as additional input angluin gasarch and smith as well as kinber smith velauthapillai and wiehagen have provide mathematical justiication for this phenomenon in the inductive inference framework however they proof rely heavily on selfreferential code trick that be they directly code the solution of the learning problem into the context fulk have show that for the exand bcanomaly hierarchy such result which rely on selfreferential code trick may not hold robustly in this work we analyze robust version of learn aid by context and show that in contrast to fulk s result above the robust version of y support by the deutsche forschungsgemeinschaft lrb dfg rrb graduiertenkolleg beherrschbarkeit komplexer systeme lrb grk 2092 96 rrb these learn notion be still very powerful also study be the diiculty of the functional dependence between the intended target task and useful associate context doi 101145 279943279952 robust version	COLT	
108784	john_case sanjay_jain arun_sharma	on learning limit program	machine learning of italic limit program italic lrb ie program allow finitely many mind change about they legitimate output rrb for italic computable italic function be study learning of italic iterated italic limit program be also study to partially motivate these study it be show that in some case interesting global property of computable function can be prove from suitable lrb italic n italic 1 rrb iterate limit program for they which can italic not italic be prove from italic any n italic iterated limit program for they it be show that learn power be increase when lrb italic n italic 1 rrb iterate limit program rather than italic n italic iterated limit program be to be learn many tradeoff result be obtain regard learn power number lrb possibly zero rrb of limit take program size constraint and number of error tolerate in final program learn doi 101145 130385130407 computable function	COLT	
108785	john_case sanjay_jain frank_stephan rolf_wiehagen	robust learning rich and poor	a class c of recursive function be call robustly learnable in the sense i lrb where i be any success criterion of learning rrb if not only c itself but even all transform class lrb c rrb where be any general recursive operator be learnable in the sense i it be already show before see lsb ful90 jsw01 rsb that for i ex lrb learn in the limit rrb robust learning be rich in that there be class be both not contain in any recursively enumerable class of recursive function and nevertheless robustly learnable for several criterion i the present paper make much more precise where we can hope for robustly learnable class and where we can not this be achieve in two way first for i ex it be show that only consistently learnable class can be uniformly robustly learnable second some other learning type i be classify as to whether or not they contain rich robustly learnable class moreover the first result on separate robust learning from uniformly robust learning be derive doi 101016 jjcss 200310005 deal robust learning recursive function enumerable learnable class	COLT/EuroCOLT	
108909	mark_a._fulk sanjay_jain	learning in the presence of inaccurate information	the present paper consider the effect of introduce inaccuracy in a learner s environment in gold s learning model of identification in the limit three kind of inaccuracy be consider presence of spurious datum be model as learn from a noisy environment miss datum be model as learn from incomplete environment and the presence of a mixture of both spurious and missing datum be model as learn from imperfect environment two learn domain be consider namely identification of program from graph of computable function and identification of grammar from positive datum about recursively enumerable language many hierarchy and tradeoff result from the interplay between the number of error allow in the final hypothesis the number of inaccuracy in the datum the type of inaccuracy and the type of success criterion be derive a interesting result be that in the context of function learning incomplete datum be strictly worse for learn than noisy datum doi 101016 03043975 lrb 95 rrb 001352 learning machine imperfect inaccuracy incomplete ers	COLT	
109004	sanjay_jain efim_b._kinber	intrinsic complexity of learning geometrical concept from positive datum	intrinsic complexity be use to measure the complexity of learn area limit by brokenstraight line lrb call open semihull rrb and intersection of such area any strategy learn such geometrical concept can be view as a sequence of primitive basic strategy thus the length of such a sequence together with the complexity of the primitive strategy use can be regard as the complexity of learn the concept in question we obtain the best possible lower and upper bound on learn open semihull as well as match upper and lower bound on the complexity of learn intersection of such area surprisingly upper bound in both case turn out to be much lower than those provide by natural learning strategy another surprising result be that learn intersection of open semihull turn out to be easier than learn open semihull themselves doi 101016 s00220000 lrb 03 rrb 000679 intrinsic complexity surprising upper and lower bounds init geometrical concept	COLT/EuroCOLT	
109005	sanjay_jain efim_b._kinber	on learning languages from positive datum and a limited number of short counterexample	foreword this technical report contain a research paper development or tutorial article which have be submit for publication in a journal or for consideration by the commission organization the report represent the idea of its author and should not be take as the official view of the school or the university any discussion of the content of the report should be send to the author at the address show on the cover abstract we consider two variant of a model for learn language in the limit from positive datum and a limited number of short negative counterexample lrb counterexample be consider to be short if they be smaller that the largest element of input see so far rrb negative counterexample to a conjecture be example which belong to the conjectured language but do not belong to the input language within this framework we explore howwhen learner use n short lrb arbitrary rrb negative counterexample can be simulated lrb or simulate rrb use least short counterexample or just no answer from a teacher we also study how a limited number of short counterexample fair against unconstrained counterexample and also compare they capability with the datum that can be obtain from subset superset and equivalence query lrb possibly with counterexample rrb a surprising result be that just one short counterexample can sometimes be more useful than any bounded number of counterexample of arbitrary size most of result exhibit salient example of language learnable or not learnable within corresponding variant of we model doi 101007 1177642021 arbitrary gnc bounded number positive datum counterexample	COLT	
109006	sanjay_jain efim_b._kinber rolf_wiehagen	language learning from texts degree of instrinsic complexity and they characterization	thesis advisor be friendly patient and always ready to share they knowledge on every matter prof reis be also the advisor during my study towards the master degree and be really influential on my decision to work as a researcher he dedication and persistence always give i encouragement to reach for higher goal prof glesner receive i in he institute for a temporary stay but soon invite i to join he staff and trust i to take a position of great responsibility always provide guidance when i most need he help i to bring my research potential to another level work with both of they be a truly rewarding experience the possibility of have two thesis advisor and a binational doctoral degree be grant by a special agreement between the universidade federal do rio grande do sul lrb ufrgs rrb and the technische universitt darmstadt lrb tud rrb i thank abdelhak zoubir be thank for the stimulate discussion during the research phase and for they valuable feedback on the thesis text and oral presentation i would like to thank my professor and colleague for the excellent work environment i have during the last five year from the microelectronic systems institute in darmstadt i would like to thank all postdoc and doctoral researcher secretary and technician as well as the diplom and master student who support i on the implementation work related to the thesis special thanks to prof dring juergen becker lrb who provide i and my family with essential support on we arrival to darmstadt rrb dring the staff of the network administration and library of informatics institute at ufrgs be also thank for be always friendly and attentive for they support on my language proficiency examination i thank bsc during the year i work on this thesis i live in four different city two different country many temporary and permanent address but my friend be always able to find i and support i i will have to thank all of they by mention only a few people from the cl finally and most importantly i would like to thank my family my parent leon and dalila who give i everything whose example i will always follow and whom i will never be able to thank enough my brother daniel and lucas for be my friend from the moment they have be bear my wife giuliana for be by my side with love and support ever since	COLT	
109007	sanjay_jain efim_b._kinber rolf_wiehagen	learning all subfunctions of a function	sublearning a model for learning of subconcept of a concept be present sublearning a class of total recursive function informally mean to learn all function from that class together with all of they subfunction while in language learn it be know to be impossible to learn any infinite language together with all of its sublanguage the situation change for sublearning of function several type of sublearning be define and compare to each other as well as to other learn type for example in some case sublearn coincide with robust learning furthermore whereas in usual function learn there be class that can not be learn consistently all sublearnable class of some natural type can be learn consistently moreover the power of sublearning be characterize in several term thereby establish a close connection to measurable class and variant of this notion as a consequence there be rich class which do not need any selfreferential code for sublearn they doi 101016 jic 200403003 re subfunction learnable finite card	COLT	
109008	sanjay_jain arun_sharma	finite learning by a team		COLT	
109009	sanjay_jain arun_sharma	probability be more powerful than team for language identification from positive datum	a team of learn machine be essentially a multiset of learn machine a team be say to successfully identify a concept just in case each member of some nonempty subset of the team identify the concept the ratio between the minimum number of team member require to be successful to the cardinality of the team be refer to as the success ratio of the team identification criterion team identification of program for computable function from they graph haa be investigate by smith pitt show that this notion be essentially equivalent to function identification by a single probabilistic machine as a consequence of this equivalence it be show by pitt and smith that introduce redundancy in a team do not yield any extra function learn power the present paper study the more difficult subject of probabilistic and team identification of grammar for language from positive datum earlier result have establish that for team success ratio 12 redundancy help in certain case result in the present paper complete the picture for team success ratio 12 and show that probabilistic identification with probability of success at least 12 be strictly more powerful than team identification with success ratio 12 with a view h cope with the complexity of diagonalization ar ent a general tool be present that ykdd new diagonalization result from simple arithmetic manipulation of the parameter of know result employ this tool on result about success ratio 12 it be show that for k 2 probabilistic idenpermission to copy without fee all or part of this material be grant provide that the copy be not make or distribute for direct commercial advantage the acm copyright notice and the title of the publication and its date appear and notce be give that copying be by permission of the association for computing machinery to copy otherwise or to republish require a fee andor specific permission tification with probability of success at least 1k be strictly more powerful than team identification with success ratio lk additionally several new general result be obtain use this tool it be also observe that for identification of language from both positive and negative datum probabilistic learning and team learning be equivalent doi 101145 168304168331 probability of success learning machine team identification success ratio tification	COLT	
109010	sanjay_jain arun_sharma	on the intrinsic complexity of language identification	a new investigation of the complexity of language identification be undertake use the notion of reduction from recursion theory and complexity theory the approach refer to as the intrinsic complexity of language identification employ notion of 8220 weak 8221 and 8220 strong 8221 reduction between learnable class of language the intrinsic complexity of several class be consider and the result agree with the intuitive difficulty of learn these class several complete class be show for both the reduction and it be also establish that the weak and strong reduction be distinct a interesting result be that the self referential class of wiehagen in which the minimal element of every language be a grammar for the language and the class of pattern language introduce by angluin be equivalent in the strong sense this study have be influence by a similar treatment of function identification by freivalds kinber and smith doi 101145 180139181150 language identification intrinsic complexity	COLT	
109011	sanjay_jain arun_sharma	elementary formal systems intrinsic complexity and procrastination	recently rich subclass of elementary formal system lrb ef rrb have be show to be identifiable in the limit from only positive datum example of these class be angluin s pattern language union of pattern language by wright and shinohara and class of language definable by lengthbounded elementary formal system study by shinohara the present paper employ two distinct body of abstract study in the inductive inference literature to analyze the learnability of the these concrete class the first approach introduce by freivalds and smith use constructive ordinal to bind the number of mind change w denote the first limit ordinal a ordinal mind change bind of u mean that identification can be carry out by a learner that after examine some element lrb s rrb of the language announce a upper bind on the number of mind change it will make before converge a bind of 2w mean that the learner reserve the right to revise this upper bind once a bind of 3w mean the learner reserve the right to revise this upper bind twice and so on a bind of w2 mean that identification can be carry out by a learner that announce a upper bind on the number of time it may revise its conjectured upper bind on the number of mind change it be show in the present paper that the ordinal mind change complexity for identification of language form by union of up to n pattern language be wn it be also show that this bind be essential similar result be also permission to make digitalhard copy of all or part of this material for personal or classroom use be grant without fee provide that rhe copy be not make or distribute for profit or commercial advantage the copyright ootice the title of the publication and its date appear and notice ia show to hold for class definable by lengthbounded elementary formal system with up to n clause the second approach study by freivalds kinber and wiehagen and by jain and sharma employ reduction to study the intrinsic complexity of learnable class it be show that the class of language form by take union of up to n 1 pattern language be a strictly more difficult learning problem than the class of language form by the union of up to n pattern language it be also show that a similar hierarchy hold for the bind on the number of doi 101145 238061238093 pattern language mind change txtex	COLT	
109012	sanjay_jain arun_sharma	on a generalized notion of mistake bounds	this paper propose the use of constructive ordinal as mistake bound in the online learning model this approach elegantly generalize the applicability of the online mistake bind model to learnability analysis of very expressive concept class like pattern language union of pattern language elementary formal system and minimal model of logic program the main result in the paper show that the topological property of effective finite bound thickness be a sufficient condition for online learnability with a certain ordinal mistake bind a interesting characterization of the online learning model be show in term of the identification in the limit framework it be establish that the class of language learnable in the online model with a mistake bind of be exactly the same as the class of language learnable in the limit from both positive and negative datum by a popperian consistent learner with a mind change bind of this result nicely build a bridge between the two model doi 101145 307400307450 mistake learnability natural number logic program ordinal	COLT	
109013	sanjay_jain arun_sharma john_case	convergence to nearly minimal size grammar by vacillate learning machine		COLT	
109014	sanjay_jain carl_h._smith rolf_wiehagen	on the power of learning robustly	1 introduction intuitively a class of object be robustly learnable if not only this class itself be learnable but all of its computable transformation do remain learnable as well in that sense be learnable robustly seem to be a desirable property in all field of learning we will study this phenomenon within the paradigm of inductive inference here a class of recursive function be call robustly learnable under a success criterion i iff all of its image under general recursive operator be learnable under the criterion i fulk lsb fu190 rsb show the existence of a nontrivial class which be robustly learnable under the criterion ex however several of the hierarchy lrb such as the anomaly hierarchy for ex and bc rrb do not stand robustly hence up to now it be not clear if robust learning be really rich the main intention of this paper be to give strong evidence that robust learning be rich we main result prove by a priority construction be that the mind change hierarchy for ex stand robustly moreover the hierarchy of team learning for both ex and bc stand robustly as well in several context we observe the surprising fact that a more complex topological structure of the class to be learn lead to positive robustness result whereas a easy topological structure yield negative result we also show the counterintuitive fact that even some selfreferential class can be learn robustly some of we result point out the difficulty of robust learning when only a bound number of mind change be allow further result concern uniformly robust learning be summarize consider the follow basic learning scenario a learn machine have to learn some unknown object that be base on some information the machine create one or more hypothesis which finally yield a correct global description of that object then consider the extent to which such a machine can be a general purpose learner in that it can learn each object from a finite or even infinite class of object in various learn model this can be show to be possible a next question to ask then could be the following how stable be the property that a class be learnable that be under which small or not so small transformation the transform class remain learnable of course this may depend on the class under consideration some class may be more stable in that doi 101145 279943279982 robust learning class of object	COLT	
109382	sanjay_jain frank_stephan	mitotic class	foreword this technical report contain a research paper development or tutorial article which have be submit for publication in a journal or for consideration by the commission organization the report represent the idea of its author and should not be take as the official view of the school or the university any discussion of the content of the report should be send to the author at the address show on the cover abstract for the natural notion of splitting class into two disjoint subclass via a recursive classifier work on text the question be address how these splitting can look in the case of learnable class here the strength of the class be compare use the strong and weak reducibility from intrinsic complexity it be show that for explanatorily learnable class the complete class be also mitotic with respect to weak and strong reducibility respectively but there be a weak complete class which can not be split into two class which be of the same complexity with respect to strong reducibility it be show that for complete class for behaviourally correct learning one half of each splitting be complete for this learn notion as well furthermore it be show that explanatorily learnable and recursively enumerable class always have a splitting into two incomparable class this give a inductive inference counterpart of sacks split theorem from recursion theory	COLT	
109403	lorenzo_carlucci john_case sanjay_jain	learning correction grammars	we investigate a new paradigm in the context of learn in the limit namely learn correction grammar for class of computably enumerable lrb ce rrb language know a language may feature a representation of it in term of two grammar the second grammar be use to make correction to the first grammar such a pair of grammar can be see as a single description of lrb or grammar for rrb the language we call such grammar correction grammar correction grammar capture the observable fact that people do correct they linguistic utterance during they usual linguistic activity we show that learn correction grammar for class of ce language in the txtexmodel lrb ie converge to a single correct correction grammar in the limit rrb be sometimes more powerful than learn ordinary grammar even in the txtbcmodel lrb where the learner be allow to converge to infinitely many syntactically distinct but correct conjecture in the limit rrb for each n 0 there be a similar learning advantage again in learn correction grammar for class of ce language but where we compare learn correction grammar that make n 1 correction to those that make n correction the concept of a correction grammar can be extend into the constructive transfinite use the idea of countingdown from notation for transfinite constructive ordinal this transfinite extension can also be conceptualize as be about learn ershovdescription for ce language for u a notation in kleene s general system lrb o o rrb of ordinal notation for constructive ordinal we introduce the concept of a ucorrection grammar where u be use to bind the number of correction that the grammar be allow to make we prove a general hierarchy result if u and v be notation for constructive ordinal such that u o v then there be class of ce language that can be txtexlearn by conjecture vcorrection grammar but not by conjecture ucorrection grammar 1 surprisingly we show that above many correction it be not possible to strengthen the hierarchy txtexlearning ucorrection grammar of class of ce language where u be a notation in o for any ordinal can be simulated by txtbclearning wcorrection grammar where w be any notation for the smallest infinite ordinal doi 101007 978354072927316 single description language l ordinal transfinite grammar	COLT	
192564	andris_ambainis sanjay_jain arun_sharma	ordinal mind change complexity of language identification	the approach of ordinal mind change complexity introduce by freivalds and smith use lrb notation for rrb constructive ordinal to bind the number of mind change make by a learning machine this approach provide a measure of the extent to which a learn machine have to keep revise its estimate of the number of mind change it will make before converge to a correct hypothesis for language in the class be learn recently this notion which also yield a measure for the difficulty of learn a class of language have be use to analyze the learnability of rich concept class the present paper further investigate the utility of ordinal mind change complexity it be show that for identification from both positive and negative datum and n 1 the ordinal mind change complexity of the class of language form by union of up to n 1 pattern language be only o notn lrb n rrb lrb where notn lrb n rrb be a notation for n be a notation for the least limit ordinal and o represent ordinal multiplication rrb this result nicely extend a observation of lange and zeugmann that pattern language can be identify from both positive and negative datum with 0 mind change existence of a ordinal mind change bind for a class of learnable language can be see as a indication of its learning tractability condition be investigate under which a class have a ordinal mind change bind for identification from positive datum it be show that a index family of language have a ordinal mind preprint submit to elsevier science change bind if it have finite elasticity and can be identify by a conservative machine it be also show that the requirement of conservative identification can be sacrifice for the purely topological requirement of mfinite thickness interaction between identification by monotonic strategy and existence of ordinal mind change bind be also investigate doi 101007 354062685925 pattern language txtex mind change ordinal finite thickness	EuroCOLT	
192590	john_case sanjay_jain mandayam_suraj	control structure in hypothesis spaces the influence on learning	in any learnability setting hypothesis be conjecture from some hypothesis space study herein be the influence on learnability of the presence or absence of certain control structure in the hypothesis space first present be control structure characterization of some rather specific but illustrative learnability result the presence of these control structure be thereby show essential to maintain full learn power then present be the main theorem each of these nontrivially characterize the invariance of a learning class over hypothesis space v and the presence of a particular projection control structure call proj in v as v have suitable instance of all denotational control structure in a sense then proj epitomize the control structure whose presence need not help and whose absence need not hinder learn power doi 101016 s03043975 lrb 00 rrb 003856 hypothesis space learnability control structure decoration proj	EuroCOLT	
192598	rusins_freivalds sanjay_jain	kolmogorov numbering and minimal identification	identification of program for computable function from they graph by algorithmic device be a well study problem in learn theory freivalds and chen consider identification of minimal and nearly minimal program for function from they graph to address certain problem in minimal identification for gdel numbering freivalds later consider minimal identification in kolmogorov numbering kolmogorov numbering be in some sense optimal numbering and have some nice property we prove certain separation result for minimal identification in every kolmogorov numbering in addition we also compare minimal identification in gdel numbering versus minimal identification in kolmogorov numbering doi 101007 3540591192177 formal definition numbering class of function minimal identification criterion	EuroCOLT	
192612	sanjay_jain arun_sharma	the structure of intrinsic complexity of learn	limit identification of re index for re language lrb from a presentation of element of the language rrb and limit identification of program for computable function lrb from a graph of the function rrb have serve as model for investigate the boundary of learnability recently a new approach to the study of intrinsic complexity of identification in the limit have be propose this approach instead of deal with the resource requirement of the learning algorithm use the notion of reducibility from recursion theory to compare and to capture the intuitive difficulty of learn various class of concept freivalds kinber and smith have study this approach for function identification and jain and sharma have study it for language identification the present paper explore the structure of these reducibility in the context of language identification it be show that there be a infinite hierarchy of language class that represent learn problem of increase difficulty it be also show that the language class in this hierarchy be incomparable under the reduction introduce to the collection of pattern language richness of the structure of intrinsic complexity be demonstrate by prove that any finite acyclic direct graph can be embed in the reducibility structure however it be also establish that this structure be not dense the question of embed any infinite acyclic direct graph be open doi 101007 3540591192176 fin infinite mind change finite weak	EuroCOLT	
192613	sanjay_jain arun_sharma	mind change complexity of learning logic programs	the present paper motivate the study of mind change complexity for learn minimal model of lengthbounded logic program it establish ordinal mind change complexity bound for learnability of these class both from positive fact and from positive and negative fact building on angluin s notion of finite thickness and wright s work on finite elasticity shinohara define the property of bound finite thickness to give a sufficient condition for learnability of index family of computable language from positive datum this paper show that a effective version of shinohara s notion of bound finite thickness give sufficient condition for learnability with ordinal mind change bind both in the context of learnability from positive datum and for learnability from complete lrb both positive and negative rrb datum let be a notation for the first limit ordinal then it be show that if a language define framework yield a uniformly decidable family of language and have effective bound finite thickness then for each natural number m 0 the class of language define by formal system of length m be identifiable in the limit from positive datum with a mind change bind of m be identifiable in the limit from both positive and negative datum with a ordinal mind change bind of m the above sufficient condition be employ to give a ordinal mind change bind for learnability of minimal model of various class of lengthbounded prolog program include shapiro s linear program arimura and shinohara s depthbounded linearlycovering program and krishna rao s depthbounded linearlymoded program it be also note that the bind for learn from positive datum be tight for the example class consider machine learning in the context of firstorder logic and its subclass can be trace back to the work of plotkin lsb plo71 rsb and shapiro lsb sha81 rsb in recent year this work have evolve into the very active field of inductive logic programming lrb ilp rrb numerous practical system have be build to demonstrate the feasibility of learn logic program as description of complex concept the utility of these system have be demonstrate in many domain include drug design protein secondary structure prediction and finite element mesh design lrb see muggleton and deraedt lsb mdr94 rsb lavrac and dzeroski lsb ld94 rsb bergadano and gunetti lsb bg96 rsb and nienhuyscheng and de wolf lsb ncdw97 rsb for a survey of this field rrb together with practical development there have also be some interest in derive learnability theorem for ilp several result doi 101016 s03043975 lrb 01 rrb 000846 mind change learnability logic program positive datum ilp	EuroCOLT	
219437	john_case sanjay_jain arun_sharma	complexity issues for vacillatory function identification	it be previously show by barzdin and podnieks that one do not increase the power of learn program for function by allow learn algorithm to converge to a finite set of correct program instead of require they to converge to a single correct program in this paper we define some new subtle but natural concept of mind change complexity for function learning and show that if one bound this complexity for learn algorithm then by contrast with barzdin and podnieks result there be interesting and sometimes complicated tradeoff between these complexity bound bound on the number of final correct program and learn power doi 101006 inco 19951013 finite set mind change function learning cardinality barzdin	FSTTCS	
219632	lane_a._hemachandra sanjay_jain	on the limitations of locally robust positive reduction	polynomialtime positive reduction as introduce by selman be by definition globally robust they be positive with respect to all oracle this paper study the extent to which the theory of positive reduction remain intact when they global robustness assumption be remove we note that twosided locally robust positive reduction reduction that be positive with respect to the oracle to which the reduction be make be sufficient to retain all crucial property of globally robust positive reduction in contrast we prove absolute and relativized result show that onesided local robustness fail to preserve fundamental property of positive reduction such as the downward closure of np doi 101007 354052048144 reducibility global robustness equality positive reduction oracle	FSTTCS	
219664	sanjay_jain	branch and bind on the network model	karp and zhang develop a general randomize parallel algorithm for solve branch and bind problem they show that with high probability they algorithm attain optimal speedup within a constant factor lrb for p n lrb log n rrb c where p be the number of processor n be the size of the problem and c be a constant rrb ranade later simplify the analysis and obtain a better processor bind karp and zhang s algorithm work on model of computation where communication cost be constant the present paper consider the branch and bind problem on network where the communication cost be high suppose send a message in a p processor network take g o lrb log p rrb time and node expansion lrb define below rrb take unit time lrb other operation be free rrb then a simple randomized algorithm be present which be asymptotically nearly optimal for p o lrb 2 log c n rrb where c be any constant 13 and n be the number of node in the input tree with cost no greater than the cost of the optimal leaf in the tree doi 101007 354060692037 number of processor communication cost optimization problem	FSTTCS	
219666	sanjay_jain efim_b._kinber	learning languages from positive datum and a finite number of queries	a computational model for learn language in the limit from full positive datum and a bounded number of query to the teacher lrb oracle rrb be introduce and explore equivalence superset and subset query be consider lrb for the latter one we consider also a variant when the learner test every conjecture but the number of negative answer be uniformly bound rrb if the answer be negative the teacher may provide a counterexample we consider several type of counterexample arbitrary least counterexample the one whose size be bound by the size of positive datum see so far and no counterexample a number of hierarchy base on the number of query lrb answer rrb and type of answerscounterexamples be establish capability of learn with different type of query be compare in most case one or two query of one type can sometimes do more than any bounded number of query of another type still surprisingly a finite number of subset query be sufficient to simulate the same number of equivalence query when behaviourally correct learner do not receive counterexample and may have unbounded number of error in almost all conjecture doi 101007 978354030538530 input language negative counterexample bounded number superset positive datum	FSTTCS	
235368	john_case mandayam_suraj sanjay_jain	notsonearlyminimalsize program inference	freivalds define a acceptable programming system independent criterion for learn program for function in which the final program be require to be both correct and nearly minimal size ie within a computable function of be purely minimal size kinber show that this parsimony requirement on final program severely limit learn power nonetheless in for example scientific inference parsimony be consider highly desirable a limcomputable function be lrb by definition rrb one computable by a procedure allow to change its mind finitely many time about its output investigate be the possibility of assuage somewhat the limitation on learn power result from require parsimonious final program by use of criterion which require the final correct program to be notsonearly minimal size eg to be within a limcomputable function of actual minimal size it be interestingly show that some parsimony in the final program be thereby retain yet learn power strictly increase also consider be limcomputable function as above but for which notation for constructive ordinal be use to bind the number of mind change allow regard the output this be a variant of a idea introduce by freivalds and smith for this ordinal complexity bound version of limcomputability the power of the resultant learning criterion form strict infinite hierarchy intermediate between the computable and the limcomputable case many open question be also present doi 101007 35406021785	GOSLER_Final_Report	
235377	sanjay_jain arun_sharma	on identification by team and probabilistic machine		GOSLER_Final_Report	
268324	sanjay_jain arun_sharma	language learning by a team lrb extended abstract rrb		ICALP	
311057	john_case sanjay_jain eric_martin arun_sharma frank_stephan	identify cluster from positive datum	the present work study clustering from a abstract point of view and investigate its property in the framework of inductive inference any class s consider be give by a numbering a 0 a 1 of nonempty subset of n or q k which be use as a hypothesis space a clustering task be a finite and nonempty set of index of pairwise disjoint set the class s be say to be clusterable if there be a algorithm which for every clustering task i converge in the limit on any text for i i a i to a finite set j of index of pairwise disjoint cluster such that j j a j i i a i a class be call semiclusterable if there be such a algorithm which find a j with the last condition relax to j j a j i i a i the relationship between natural topological property and clusterability be investigate topological property can provide sufficient or necessary condition for clusterability but they can not characterize it on one hand many interesting condition make use of both the topological structure of the class and a wellchosen numbering on the other hand the clusterability of a class do not depend on the decision which numbering of the class be use as a hypothesis space for the clusterer these idea be demonstrate in the context of geometrically define class clustering of many of these class require besides the text for the clustering task some additional information the class of convex hull of finitely many point in a rational vector space can be cluster with the number of cluster as additional information interestingly the class of polygon lrb together with they interior rrb be clusterable if the number of cluster and the overall number of vertex of these cluster be give to the clusterer as additional information intriguingly this additional information be not sufficient for class include figure with hole while some class be unclusterable due to they topological structure other be only computationally intractable oracle can be use to distinguish between both case the former can not be cluster use any oracle while the latter can be cluster use some oracle it be show that there be maximal oracle that allow clustering of all problem that can be cluster with the help of some oracle in particular e be maximal iff e t k e doi 101137 050629112 hypothesis space clusterer disjoint number of cluster clusterability	ICGI	
343558	thomas_j._leblanc sanjay_jain	crowd control coordinate process in parallel		ICPP	
483927	ganesh_baliga john_case sanjay_jain mandayam_suraj	machine learning of higher order programs	a generator program for a computable function lrb by definition rrb generate a infinite sequence of program all but finitely many of which compute that function machine learning of generator program for computable function be study to partially motivate these study it be show that in some case interesting global property for computable function can be prove from suitable generator program which can not be prove from any ordinary program for they the power lrb for variant of various learn criterion from the literature rrb of learn generator program be compare with the power of learn ordinary program the learn power in these case be also compare to that of learn limit program ie program allow finitely many mind change about they correct output doi 101007 bfb0023859 computable function generator program ordinary programs	LFCS	
484009	lane_a._hemachandra sanjay_jain nikolai_k._vereshchagin	banish robust turing completeness	this paper prove that promise class be so fragilely structure that they do not robustly lrb ie with respect to all oracle rrb possess turinghard set even in class far larger than themselves in particular this paper show that fewp do not robustly possess turing hard set for up coup and ip coip do not robustly possess turing hard set for zpp and ip coip do not robustly possess turing complete set this both resolve open question of whether promise class lack robust downward closure under ture reduction lrb eg r up fewp rrb might robustly have ture complete set and extend the range of class know not to robustly contain manyone complete set doi 101007 bfb0023873 complete set fewp promise class	LFCS	
526454	chu-cheow_lim yoke-hean_low boon-ping_gan sanjay_jain wentong_cai wen-jing_hsu shell-ying_huang	performance prediction tools for parallel discreteevent simulation	we have develop a set of performance prediction tool which help to estimate the achievable speedup from parallelize a sequential simulation the tool focus on two important factor in the actual speedup of a parallel simulation program lrb a rrb the simulation protocol use and lrb b rrb the inherent parallelism in the simulation model the first two tool be a performanceparallelism analyzer for a conservative asynchronous simulation protocol and a similar analyzer for a conservative synchronous lrb superstep rrb protocol each analyzer allow we to study how the speedup of a model change with increase number of processor when a specific protocol be use the third tool a critical path analyzer give a ideal upper bind to the model s speedup this paper give a overview of the prediction tool and report the prediction from apply the tool to a discreteevent wafer fabrication simulation model the prediction be close to speedup from actual parallel implementation these tool help we to set realistic expectation of the speedup from a parallel simulation program and to focus we work on issue which be more likely to yield performance improvement conservative analyzer speedup prediction tool parallel simulation	Workshop_on_Parallel_and_Distributed_Simulation	
526477	boon-ping_gan yoke-hean_low sanjay_jain stephen_john_turner wentong_cai wen-jing_hsu shell-ying_huang	load balancing for conservative simulation on share memory multiprocessor system	italic load balancing be a crucial factor in achieve good performance for parallel discrete event simulation in this paper we present a load balancing scheme that combine both static partitioning and dynamic load balancing the static partition scheme map simulation object to logical process before simulation start while the dynamic load balancing scheme attempt to balance the load during runtime the static scheme involve two step first the simulation object that contribute to small lookahead be merge together by use a merge algorithm then a partition algorithm be apply the merge be need to ensure a consistent performance for we dynamic scheme we dynamic scheme be tailormade for a asynchronous simulation protocol that do not rely on null message the performance study on a supplychain simulation show that the partition algorithm and dynamic load balancing be important in achieve good performance italic merge static partitioning communication cost simulation entity dynamic load balancing	PADS	Gintic_Institute_of_Manufacturing_Technology 71_Nanyang_Drive Singapore_638075
529009	chee-kong_chui sanjay_jain raghu_raghavan	topology independent model for parallel computation		Parcella	
551516	sanjay_jain arun_sharma	team learning of recursive languages		PRICAI	
619840	ganesh_baliga john_case sanjay_jain	language learning with some negative information	gold style language learning be a formal theory of learn from example by algorithmic device call learn machine originally motivate by child language learning it feature the algorithmic synthesis lrb in the limit rrb of grammar for formal language from information about those language in traditional gold style language learning learn machine be not provide with negative information ie information about the complement of the input language we investigate two approach to provide small amount of negative information and demonstrate in each case a strong result increase in learn power finally we show that small packet of negative information also lead to increase speed of learning this result agree with a psycholinguistic hypothesis of mcneill correlate the availability of parental expansion with the speed of child language development doi 101006 jcss 19951066 txtex txtbc learning power positive information negative information	STACS	
620043	john_case keh-jiann_chen sanjay_jain	cost of general purpose learning	leo harrington surprisingly construct a machine which can learn any computable function f accord to the follow criterion lrb call bc identification rrb he machine on the successive graph point of f output a corresponding infinite sequence of program p 0 p each compute a variant of f which differ from f at only finitely many argument place a machine with this property be call general purpose the sequence for harrington s general purpose machine for distinct m and n the finitely many argument place where p i m fail to compute f can be very different from the finitely many argument place where p i n fail to compute f one would hope though that if harrington s machine or a improvement thereof infer the program p i m base on the datum point f lrb 0 rrb f lrb 1 rrb f lrb k rrb then p i m would make very few mistake compute f at the near future argument k 1 k 2 k where be reasonably large ideally p i m s finitely many mistake or anomaly would lrb mostly rrb occur at argument x k ie ideally its anomaly would be well place beyond near future argument in the present paper for general purpose learn machine it be analyze just how well or badly place these anomaly may be with respect to near future argument and what be the various tradeoff in particular there be good news and bad bad news be that for any learn machine m lrb include general purpose m rrb for all m there exist infinitely many computable function f such that infinitely often m incorrectly predict f s next m near future value good news be that for a suitably clever general purpose learn machine m for each computable f for m on f the density of any such associated bad prediction interval of size m be vanishingly small consider too be the possibility of provide a general purpose learner which additionally learn some interesting class with respect to much stricter criterion than bc identification again there be good news and bad the criterion of finite identification require for success that a learner m on a function f output exactly one program which correctly compute f bc nidentification be just like bc identification above except that the number of anomaly in each program of a final sequence be n bad news be that doi 101016 s03043975 lrb 00 rrb 000281 computable general purpose function f anomaly pro	STACS	
625959	sanjay_jain jochen_nessel frank_stephan	invertible class	foreword this technical report contain a research paper development or tutorial article which have be submit for publication in a journal or for consideration by the commission organization the report represent the idea of its author and should not be take as the official view of the school or the university any discussion of the content of the report should be send to the author at the address show on the cover abstract this paper consider when one can invert general recursive operator which map a class of function f to f in this regard we study four different notion of inversion we additionally consider enumeration of operator which cover all general recursive operator which map f to f in the sense that for every general recursive operator mapping f to f there be a general recursive operator in the enumerated sequence which behave the same way as on f three different possible type of enumeration be study	TAMC	
625961	sanjay_jain frank_stephan	some recent result in ushaped learning	ushaped learning deal with a learner first have the correct hypothesis then change it to a incorrect hypothesis and then relearn the correct hypothesis this phenomenon have be observe by psychologist in various study of child development in this survey talk we will discuss some recent result regard ushaped learning and related criterion a language be a set of sentence use word over a alphabet sentence and word over a alphabet can be encode into natural number thus one may model a language as a subset of n the set of natural number consider the follow model of learn a language a learner over time receive one by one element of the language in arbitrary order as the learner be receive the datum it conjecture a sequence of grammar g 0 g 1 potentially describe the input language one may consider the learner to be successful if this sequence of conjecture eventually stabilize to a grammar g lrb ie beyond certain point all its conjecture be the grammar g rrb and this grammar g be a indeed a grammar for the input language in we model we take the learner to be computable this criterion of success originate with gold lsb 16 rsb and be refer to as txtex learning lrb txt stand for text and ex stand for explanatory learning rrb note here that the learner only get datum about what be in the language and be not tell about what be not in the language thus such kind of learning be often call learn from positive datum it be not so interesting to consider learning of just one language as a learner which just output the grammar for the single language will ofcourse be able to learn it thus one usually consider learnability of a class l of language where the learner be require to learn all the language l in the class from all possible text for the language l lrb here a text for l be presentation of all and only the element of l in arbitrary order rrb this model of learning be first introduce doi 101007 1175032140	TAMC	
626800	sanjay_jain arun_sharma	hypothesis formation and language acquisition with a infinitelyoften correct teacher	the presence of a infinitelyoften correct teacher in scientific inference and language acquisition be motivate and study the treatment be abstract in the practice of science a scientist perform experiment to gather experimental datum about some phenomenon and then try to construct a explanation lrb or theory rrb for the phenomenon a model for the practice of science be a inductive inference machine lrb a scientist rrb learn a program lrb a explanation rrb from the graph lrb set of experiment rrb of a recursive function lrb phenomenon rrb it be argue that this model of science be not a adequate one as scientist in addition to perform experiment make use of some approximate explanation lrb base on the state of the art rrb about the phenomenon under investigation a attempt have be make to model this approximate explanation as a additional information in the scientific process it be show that inference power of machine be improve in the presence of a approximate explanation the quality of this approximate information be model use certain density notion it be show that additional information about a better quality approximate explanation enhance the inference power of learn machine as scientist more than a not so good approximate explanation inadequacy in gold s paradigm of language learning be investigate it be argue that gold s model fail to incorporate any additional information that child get from they environment child be sometimes tell about some grammatical rule that enumerate element of the language these rule be some sort of additional information also child be be give some information about what be not in the language sometimes they be rebuke for make incorrect utterance or be tell of a rule that enumerate certain nonelement of the language a attempt have be make to extend gold s model to incorporate both these kind of additional information it be show that either type of additional information enhance the learn power of formal language learn device language learning succession inductive inference anomaly natural number	TARK	
667715	jerry_banks sanjay_jain stephen_j._buckley peter_lendermann mani_s._manivannan	supply chain opportunity panel session opportunity for simulation in supply chain management	it have become a matter of survival that many company improve they supply chain efficiency this present a opportunity for simulation however there be many challenge that must be overcome for simulation to be a contributor to play a effective role four contributor discuss the opportunity that they see for simulation to play a meaningful role in the area of supply chain management contributor supply chain management	Winter_Simulation_Conference	Virginia_Tech Alexandria VA
668672	boon-ping_gan li_liu sanjay_jain stephen_john_turner wentong_cai wen-jing_hsu	manufacturing sypply chain management distribute supply chain simulation across enterprise boundary	the effective practice of supply chain management lrb scm rrb be crucial to improve corporation competitive advantage many corporation have build simulation model to facilitate the application of simulation in design evaluate and optimize they supply chain traditionally a supply chain involve only a single enterprise with multiple facility and distribution center hence sharing of detailed simulation model be not a problem in this scenario but in recent year the scope of scm have evolve to cross the enterprise boundary apply simulation in design evaluate and optimize the supply chain become more difficult since the participate corporation might not be willing to share they simulation model with partner in this paper distribute simulation technique be present as a enable technology that allow corporation to construct a cross enterprise simulation while hide model detail within the enterprise this can be realize by either build the simulation on top of the runtime infrastructure of the high level architecture or build the simulation on top of a customize distribute discrete event simulation protocol these alternative approach be compare in term of they performance and interoperability the comparison of the performance be do through a benchmark test of a semiconductor supply chain model enterprise boundaries supply chain simulation model scm corporation	Winter_Simulation_Conference	Gintic_Institute_of_Manufacturing_Technology SINGAPORE_638075
669170	sanjay_jain	supply chain management tradeoffs analysis	supply chain management involve understand complex interaction between many factor and use the understand to achieve balance between conflicting objective simulation be a very useful technique to evaluate the impact of change in factor such as inventory control and business process parameter this paper describe a simulation base study for analyze the tradeoff among service level inventory and lead time for a large logistics supply chain the study highlight the use of simulation in understand seemingly nonintuitive result and guide the effort for performance improvement service level supply chain operating region conflicting objectives control parameter	Winter_Simulation_Conference	
669172	sanjay_jain	simulation in the next millennium	this paper look into the future of simulation the capability and feature of simulation software of the future be hypothesize focus on the major aspect of they use the feature address include model building visualization output analysis and optimization integration and the internet the discussion of simulation software in the future be closely related to the vision of future application of simulation application of simulation in the near term future be propose use example of advanced application that be in conceptual stage at present application of simulation in the long term future be also propose border on science fiction doi 101145 324898325306 simulation software	Winter_Simulation_Conference	Gintic_Institute_of_Manufacturing_Technology 71_Nanyang_Drive Singapore_638075
669173	sanjay_jain karon_barber david_osterfeld	expert simulation for online scheduling	the stateoftheart in manufacturing have move toward flexibility automation and integration the effort spend on bring computerintegrated manufacturing lrb cim rrb to plant floor have be motivate by the overall thrust to increase the speed of new product to market one of the link in cim be plant floor scheduling which be concern with efficiently orchestrate the plant floor to meet the customer demand and respond quickly to change on the plant floor and change in customer demand the expert system scheduler lrb ess rrb have be develop to address this link in cim the scheduler utilize realtime plant information to generate plant floor schedule which honor the factory resource constraint while take advantage of the flexibility of its component the scheduler use heuristic develop by a experienced human factory scheduler for most of the decision involve in scheduling the expertise of the human scheduler have be build into the computerized version use the expert system approach of the discipline of artificial intelligence lrb ai rrb deterministic simulation concept have be use to develop the schedule and determine the decision point as such simulation modeling and ai technique share many concept and the two discipline can be use synergistically example of some common concept be the ability of entity to carry attribute and change dynamically lrb simulation 8212 entitiesattributes or transactionparameters versus ai 8212 framesslots rrb the ability to control the flow of entity through a model of the system lrb simulation 8212 conditional probability versus ai 8212 production rule rrb and the ability to change the model base upon state variable lrb simulation 8212 language construct base on variable versus ai 8212 patterninvoked program rrb shannon lsb 6 rsb highlight similarity and difference between conventional simulation and a ai approach kusiak and chen lsb 3 rsb report increase use of simulation in development of expert system ess use the synergy between ai technique and simulation modeling to generate schedule for plant floor advanced concept from each of the two area be use in this endeavor the expert system have be develop use frame and objectoriented coding which provide knowledge representation flexibility the concept of 8220 backward 8221 simulation similar to the ai concept of backward chain be use to construct the event in the schedule some portion of the schedule be construct use forward or conventional simulation the implementation of expert system and simulation concept be intertwine in ess however the application of the concept from these two area will be treat separately for ease of presentation we will first discuss the expert system approach and provide a flavor of the heuristic the concept of backward simulation and the motive behind it will then be explore along with some detail of the implementation and the plant floor where the scheduler be currently be use we will then highlight some advantage and disadvantage of use the expert simulation approach for scheduling and finally the synergetic relationship between expert system and simulation doi 101145 8453784547 expert systems plant floor manufacturing scheduler ess	Winter_Simulation_Conference	
669174	sanjay_jain stephen_chan	experience with backward simulation base approach for lot release planning	bottleneck base scheduling approach suggest plan the release of lot base on the capacity of the bottleneck machine in the manufacturing process many scheduling system determine the start of the first operation of the lot base on backward schedule from its operation on the bottleneck machine a approach be develop for determine the lot release time base on backward simulation while conceptually appealing and successful with simple problem the approach do not lead to expect improvement in a highly complex semiconductor manufacturing scenario this paper describe the approach its implementation and the limitation realize in the complex scenario doi 101145 268437268622 semiconductor industry manufacturing competitiveness alternate heuristic schedule approach	Winter_Simulation_Conference	Gintic_Institute_of_Manufacturing_Technology
669175	sanjay_jain ngai_fong_choong william_g._k._lee	manufacturing supply chain application modeling computer assembly operation for supply chain integration	factory operation have be model for year to understand the relationship between the different design and policy factor and the performance measure of interest the increase awareness of the need to manage factory as a link in the supply chain place a corresponding requirement for a enhance approach for factory modeling this paper describe the modeling of a computer assembly factory for supply chain integration by include aspect of inbound and outbound logistics and relevant business process lesson be draw base on the experience responsiveness manufacturing supply chain factory business processes	Winter_Simulation_Conference	Virginia_Polytechnic_Institute_and_State_University Alexandria VA
669176	sanjay_jain eric_c._ervin andrew_p._lathrop russell_w._workman lisa_m._collins	analyze the supply chain for a large logistics operation use simulation	this paper present a case study of use simulation for analyze the impact of propose change in the supply chain process for a large logistics operation the major change explore include business process change and use of new supply chain software the result of the analysis indicate that the change in forecast accuracy provide much larger savings compare to process automation change a number of insight be draw from the result of the analysis supply chain manufacturing	Winter_Simulation_Conference	
669177	sanjay_jain swee_leong	stress testing a supply chain use simulation	simulation provide the capability to evaluate performance of a system operate under current or propose configuration policy and procedure it be very applicable to evaluation of strategic and operational level plan for supply chain it be especially useful for explore the viability of a supply chain before begin production this paper describe the use of simulation for determine the readiness of a supply chain design by a small company for provide subsystem to a defense contractor the supply chain have to be stress test at surge and mobilization volume level to meet the requirement simulation provide the ideal methodology to identify and demonstrate the behavior of the supply chain under stress and evaluate strategy to meet the define volume it be use to prove the readiness of the supply chain reduce perceive risk in use the virtual operation sme surge stress test supply chain quirement readiness	Winter_Simulation_Conference	The_George_Washington_University NW_Washington DC
669178	sanjay_jain chu-cheow_lim boon-ping_gan yoke-hean_low	criticality of detailed modeling in semiconductor supply chain simulation	supply chain management offer a large potential for organization to reduce cost and improve customer service performance simulation of supply chain can help in these objective by evaluate the impact of alternate inventory control policy supply chain simulation involve modeling of multiple factory across the chain and can get quite complex analyst typically carry out such simulation at a coarse level of detail to keep the complexity and computing resource manageable however modeling at coarse level may reduce the accuracy of output and affect the quality of decision in this paper we report on a study to compare the quality of result at different level of detail in a semiconductor supply chain simulation doi 101145 324138324547 mod simu detailed model supply chain criticality	Winter_Simulation_Conference	Gintic_Institute_of_Manufacturing_Technology 71_Nanyang_Drive Singapore_638075
669179	sanjay_jain charles_mclean	simulation for emergency response a framework for modeling and simulation for emergency response	a number of modeling and simulation tool have be develop and more be be develop for emergency response application the available simulation tool be mean mostly for standalone use address a emergency incident require address multiple interdependent aspect of the situation the simulation tool address different aspect of a emergency situation need to be integrate to provide the whole picture to planner trainer and responder a framework be require to ensure that modeling and simulation tool can be systematically integrate together to address the overall response this paper propose a framework for integration of modeling simulation and visualization tool for emergency response the development and implementation of the propose framework will significantly improve the nation s capability in the emergency response area doi 101145 10308181030960 responder model and simulation emergency response simulation application simulation tool	Winter_Simulation_Conference	Virginia_Polytechnic_Institute_and_State_University Falls_Church VA
669180	sanjay_jain charles_r._mclean	integrated simulation and gaming architecture for incident management training	the simulationbased training system that be available or under development today for incident management be typically focus on macro level sequence of event a few system target at individual responder be under development use a gaming environment separate use of such system provide disparate experience to decision maker and individual responder there be a need to provide common training experience to these group for better effectiveness this paper present a novel approach integrate gaming and simulation system for training of decision maker and responder on the same scenario prepare they to work together as a team a integrate system architecture be propose for this purpose major module in gaming and simulation subsystem be define and interaction mechanism establish research and standard issue for implementation of the propose architecture be discuss gaming responder game engine same scenario emergency incident management decision maker preparedness	Winter_Simulation_Conference	The_George_Washington_University Washington DC
669181	sanjay_jain russell_w._workman lisa_m._collins eric_c._ervin andrew_p._lathrop	supply chain application ii development of a highlevel supply chain simulation model	this paper describe a effort that involve development of a simulation model for evaluate the business process and inventory control parameter of a logistics and distribution supply chain a generic simulation tool rather than a supply chain simulator be develop for meeting customize need of the effort the paper describe the approach use to model at the select level of abstraction the development of interface for datum and experimentation and the development and delivery of animation for communicate the approach and result to the client simulation model level of detail objective of the study	Winter_Simulation_Conference	
669652	chu-cheow_lim yoke-hean_low boon-ping_gan sanjay_jain	implementation of dispatch rules in parallel manufacturing simulation	most feature in commercial simulation package be often omit in parallel simulation benchmark because they neither affect the overall correctness of the simulation protocol nor the benchmark s performance in we work on parallel simulation of a wafer fabrication plant we however find several feature which complicate the implementation of the simulation protocol and affect the program performance one such feature be the dispatch rule which a machine set use to decide the priority of the wait wafer lot in a sequential simulation the dispatch rule can be implement in a straightforward fashion because the whole system state be at the same simulation time and the rule simply read the state variable lrb of any machine resource etc rrb in a parallel simulation the dispatch rule computation may be complicate by the fact that different portion of the simulated system can be at different simulation time this paper describe we study of the implementation of dispatch rule in parallel simulation we note that this be actually a instance of the littlestudied problem of provide sharedstate information in parallel simulation we briefly survey previous related work we then outline two different approach for a dispatch rule to access the sharedstate information and compare they in term of they ease of implementation parallel simulation simulation protocol	Winter_Simulation_Conference	
671703	sanjay_jain charles_r._mclean	a concept prototype for integrate gaming and simulation for incident management	this paper describe a prototype that have be develop to demonstrate the concept of integrate gaming and simulation for incident management a architecture for the purpose be develop and present at the last conference a hypothetical emergency incident scenario have be develop for demonstrate the applicability of integrate simulation and gaming a number of simulation and gaming module have be utilize to model the major aspect of the hypothetical scenario the module demonstrate the value of utilize simulation for incident management application they can be use to highlight the value of simulation and gaming for training application in particular two of the simulation module have be integrate use a modify implementation of the high level architecture to give a idea of the advantage technical issue in integration be identify gaming incident management simulation module	Winter_Simulation_Conference	The_George_Washington_University Funger_Hall Washington DC
671980	sanjay_jain frank_riddick andreas_craens deogratias_kibira	distribute simulation for interoperability testing along the supply chain	the need for interoperability of information system among supply chain partner have be recognize a number of standard have be or be be develop to ensure interoperability of application use along the supply chain a associate need for interoperability testing have emerge there be a need to evaluate compliance of application to standard across specific platform the standard themselves need to be evaluate for a comprehensive coverage of the application scope lrb validation testing rrb this paper report on a distribute simulation base approach for supply chain interoperability testing simulation be use to represent real life organization to serve as source and consumer of dynamic datum the datum can be encapsulate per the standard under consideration and exchange with other organization directly or through select application for testing error free performance of the simulated system over time will provide confidence in the interoperability of application and in the standard themselves supply chain interoperability testing interoperability of application	Winter_Simulation_Conference	
672152	sanjay_jain	tradeoff in build a generic supply chain simulation capability	build a simulation model for any large complex system require high expertise and effort these requirement can be reduce through building generic simulation capability that include artifact for facilitate the development of the simulation model the artifact can have a range of capability depend on the design goal for the simulation this paper focus on issue to be consider in build a generic simulation capability for supply chain a number of approach use in recent year for build generic supply chain simulation capability be discuss such approach include datadriven simulator interactive simulator and submodel for supply chain component tradeoff be identify that should be consider in select a approach for build a generic supply chain simulation capability simulation model	Winter_Simulation_Conference	The_George_Washington_University Washington DC
718831	charles_mclean sanjay_jain frank_riddick y._tina_lee	a simulation architecture for manufacture interoperability testing	manufacturing system be often costly to develop and operate simulation technology have be demonstrate to be a effective tool for improve the efficiency of manufacture system design operation and maintenance but manufacture simulation be usually develop to address a narrow set of industrial issue eg the purchase of new equipment or the modification of a manufacturing process once the analysis be complete a particular simulation model may not be use again if simulation could be make more modular and easily integrate they could have tremendous value as tool for manufacture interoperability testing this paper present a modular reference architecture to facilitate the integration of manufacture simulation system with other support and testing application opportunities for testing be also discuss that will be enable by the implementation of the architecture 1 introduction manufacturing system tend to be large complex and expensive to construct and operate due to hardwareacquisition maintenance and space cost academic and research institution can not afford to duplicate real manufacturing system in they laboratory student and researcher handson experience with manufacture system be often limit to individual or small group of machine tool in laboratory shop prototype work cell or tabletop manufacturing system manufacturing research and testing could be significantly enhance if manufacture system could somehow be bring into the laboratory of academic and research institution computer simulation technology now allow we to construct large realistic virtual world in software the military and the entertainment industry have make extensive use of this technology for a number of year the industrial world be just begin to recognize the potential of this technology virtual manufacturing enterprise could be use by a variety of organization involve in manufacture for research testing and training this paper focus on how this may be apply to manufacture interoperability testing doi 101145 13579101358004 research institution manufacturing nist test case data academic	SCSC	National_Institute_of_Standards_and_Technology Gaithersburg MD
718898	sanjay_jain charles_r._mclean y._tina_lee	towards standard for integrate gaming and simulation for incident management	simulation and gaming can support decision make through all phase of incident management include prevention preparedness response recovery and mitigation a number of gaming and simulation tool have be develop for the purpose but they generally utilize proprietary or unique datum interface and they own view of partition the application and solution space this create a large obstacle for wide use and in particular prevent the use of these tool in a integrated manner to address the application space this paper explore the groundwork need to build standard for integrate gaming and simulation tool for incident management a architecture have be propose to identify the required group of simulation and gaming module for incident management and define they scope in the solution space a conceptual model be propose for the datum require for such simulation available datum exchange standard be map to the conceptual datum model a concept prototype have be develop base on the architecture to demonstrate the value of integrate modeling and simulation and the architecture itself a number of simulation and gaming module have be utilize to model the major aspect of a hypothetical scenario the exercise be use to identify the issue due to lack of datum exchange standard 1 introduction there be a grow need for preparedness for emergency response both for manmade and natural disaster event the manmade disaster risk have increase due to a rise in possibility of terrorist attack against the united states effective emergency response present a number of challenge to the responsible agency one major challenge be the lack of opportunity to train the emergency responder and the decision maker in deal with the doi 101145 13579101358099 gaming and simulation data exchange incident management emergency response preparedness	SCSC	The_George_Washington_University Washington DC
722687	sanjay_jain	hypothesis space for learning	in this paper we survey some result in inductive inference show how learnability of a class of language may depend on the hypothesis space choose additionally optimal hypothesis space usable for every learnable class be consider we also discuss result which consider how learnability be effect if one require learn use every suitable hypothesis space doi 101007 97836420098224 finite set hypothesis space class of languages learnability acceptable programming	LATA	Department_of_Computer_Science National_University_of_Singapore Singapore Republic_of_Singapore_117590
751716	sanjay_jain arun_sharma	generalization and specialization strategy for learning re languages	overgeneralization be a major issue in the identification of grammar for formal language from positive datum different formulation of generalization and specialization strategy have be propose to address this problem and recently there have be a flurry of activity investigate such strategy in the context of index family of recursive language the present paper study the power of these strategy to learn recursively enumerable language from positive datum in particular the power of strongmonotonic monotonic and weakmonotonic lrb together with they dual notion modeling specialization rrb strategy be investigate for identification of re language these investigation turn out to be different from the previous investigation on learn index family of recursive language and at time require new proof technique a complete picture be provide for the relative power of each of the strategy consider a interesting consequence be that the power of weakmonotonic strategy be equivalent to that of conservative strategy this result parallel the scenario for index class of recursive language it be also show that any identifiable collection of re language can also be identify by a strategy that exhibit the dual of weakmonotonic property a immediate consequence of the proof of this result be that if attention be restricted to infinite re language then conservative strategy can identify every identifiable collection doi 101023 a 1018903922049 recursive languages guage re languages dual grammar	Ann._Math._Artif._Intell.	
753821	lance_fortnow william_i._gasarch sanjay_jain efim_b._kinber martin_kummer stuart_a._kurtz mark_pleszkovich theodore_a._slaman robert_solovay frank_stephan	extreme in the degree of inferability		Ann._Pure_Appl._Logic	
753993	sanjay_jain arun_sharma	characterize language identification in term of computable numbering	identification of program for computable function from they graph and identification of grammar lrb r e index rrb for recursively enumerable language from positive datum be two extensively study problem in the recursion theoretic framework of inductive inference that only those collection of function s be identifiable in the limit for which there exist 1 a 1 1 computable numbering and a discrimination function d such that lrb a rrb for each f s the number of index i such that i f be exactly one and lrb b rrb for each f s there be only finitely many index i such that f and i agree on the first d lrb i rrb argument a similar characterization for language identification in the limit have turn out to be difficult a partial answer be provide in this paper several new technique be introduce which have find use in other investigation on language identification doi 101016 s01680072 lrb 95 rrb 00064x language identification number of indices numbering positive datum grammar	Ann._Pure_Appl._Logic	
754513	john_case keh-jiann_chen sanjay_jain wolfgang_merkle james_s._royer	generality s price inescapable deficiency in machinelearned program	this paper investigate some delicate tradeoff between the generality of a algorithmic learning device and the quality of the program it learn successfully there be result to the effect that thanks to small increase in generality of a learning device the computational complexity of some successfully learn program be provably unalterably suboptimal there be also result in which the complexity of successfully learn program be asymptotically optimal and the learn device be general but still thanks to the generality some of those optimal learn program be provably unalterably information deficient in some case deficient as to safe algorithmic extractabilityprovability of the fact that they be even approximately optimal for these result the safe algorithmic method of information extraction will be by proof in arbitrary true computably axiomatizable extension of peano arithmetic doi 101016 japal 200506013 learning device	Ann._Pure_Appl._Logic	
768999	areejit_samal shalini_singh varun_giri sandeep_krishna nandula_raghuram sanjay_jain	low degree metabolite explain essential reaction and enhance modularity in biological network	background recently there have be a lot of interest in identify module at the level of genetic and metabolic network of organism as well as in identify single gene and reaction that be essential for the organism a goal of computational and system biology be to go beyond identification towards a explanation of specific module and essential gene and reaction in term of specific structural or evolutionary constraint result in the metabolic network of escherichia coli saccharomyces cerevisiae and staphylococcus aureus we identify metabolite with a low degree of connectivity particularly those that be produce andor consume in just a single reaction use flux balance analysis lrb fba rrb we also determine reaction essential for growth in these metabolic network we find that most reaction identify as essential in these network turn out to be those involve the production or consumption of low degree metabolite apply graph theoretic method to these metabolic network we identify connected cluster of these low degree metabolite the gene involve in several operon in e coli be correctly predict as those of enzyme catalyze the reaction of these cluster furthermore we find that larger size cluster be overrepresented in the real network and be analogous to a network motif use fba for the above mention three organism we independently identify cluster of reaction whose flux be perfectly correlate we find that the composition of the latter functional cluster be also largely explain in term of cluster of low degree metabolite in each of these organism conclusion we finding mean that most metabolic reaction that be essential can be tag by one or more low degree metabolite those reaction be essential because they be the only way of produce or consume they respective tag metabolite furthermore reaction whose flux be strongly correlate can be think of as glue together by these low degree metabolite the method develop here could be use in predict essential reaction and metabolic module in other organism from the list of metabolic reaction doi 101186 147121057118 ule essential fba metabolic network low degree	BMC_Bioinformatics	
773097	sanjay_jain karon_barber david_osterfeld	expert simulation for online scheduling	the stateoftheart in manufacturing have move toward flexibility automation and integration the effort spend on bring computerintegrated manufacturing lrb cim rrb to plant floor have be motivate by the overall thrust to increase the speed of new product to market one of the link in cim be plant floor scheduling which be concern with efficiently orchestrate the plant floor to meet the customer demand and respond quickly to change on the plant floor and change in customer demand the expert system scheduler lrb ess rrb have be develop to address this link in cim the scheduler utilize realtime plant information to generate plant floor schedule which honor the factory resource constraint while take advantage of the flexibility of its component the scheduler use heuristic develop by a experienced human factory scheduler for most of the decision involve in scheduling the expertise of the human scheduler have be build into the computerized version use the expert system approach of the discipline of artificial intelligence lrb ai rrb deterministic simulation concept have be use to develop the schedule and determine the decision point as such simulation modeling and ai technique share many concept and the two discipline can be use synergistically example of some common concept be the ability of entity to carry attribute and change dynamically lrb simulation 8212 entitiesattributes or transactionparameters versus ai 8212 framesslots rrb the ability to control the flow of entity through a model of the system lrb simulation 8212 conditional probability versus ai 8212 production rule rrb and the ability to change the model base upon state variable lrb simulation 8212 language construct base on variable versus ai 8212 patterninvoked program rrb shannon lsb 6 rsb highlight similarity and difference between conventional simulation and a ai approach kusiak and chen lsb 3 rsb report increase use of simulation in development of expert system ess use the synergy between ai technique and simulation modeling to generate schedule for plant floor advanced concept from each of the two area be use in this endeavor the expert system have be develop use frame and objectoriented coding which provide knowledge representation flexibility the concept of 8220 backward 8221 simulation similar to the ai concept of backward chain be use to construct the event in the schedule some portion of the schedule be construct use forward or conventional simulation the implementation of expert system and simulation concept be intertwine in ess however the application of the concept from these two area will be treat separately for ease of presentation we will first discuss the expert system approach and provide a flavor of the heuristic the concept of backward simulation and the motive behind it will then be explore along with some detail of the implementation and the plant floor where the scheduler be currently be use we will then highlight some advantage and disadvantage of use the expert simulation approach for scheduling and finally the synergetic relationship between expert system and simulation doi 101145 8453784547 expert systems plant floor manufacturing scheduler ess	Commun._ACM	General_Motors_Technical_Center
861433	john_case sanjay_jain rudiger_reischuk frank_stephan thomas_zeugmann	a polynomial time learner for a subclass of regular patterns	present be a algorithm lrb for learn a subclass of erase regular pattern language rrb which can be make to run with arbitrarily high probability of success on extended regular language generate by pattern of the form x 0 1 x 1 m x m for unknown m but known c from number of example polynomial in m lrb and exponential in c rrb where x 0 x m be variable and where 1 m be each string of terminal of length c this assume that the algorithm randomly draw sample with natural and plausible assumption on the distribution with the aim of find a better algorithm we also explore computer simulation of a heuristic terminal pattern language poly prob pac	Electronic_Colloquium_on_Computational_Complexity_(ECCC)	
861453	john_case sanjay_jain eric_martin arun_sharma frank_stephan	identify cluster from positive datum	the present work study clustering from a abstract point of view and investigate its property in the framework of inductive inference any class s consider be give by a numbering a 0 a 1 of nonempty subset of n or q k which be use as a hypothesis space a clustering task be a finite and nonempty set of index of pairwise disjoint set the class s be say to be clusterable if there be a algorithm which for every clustering task i converge in the limit on any text for i i a i to a finite set j of index of pairwise disjoint cluster such that j j a j i i a i a class be call semiclusterable if there be such a algorithm which find a j with the last condition relax to j j a j i i a i the relationship between natural topological property and clusterability be investigate topological property can provide sufficient or necessary condition for clusterability but they can not characterize it on one hand many interesting condition make use of both the topological structure of the class and a wellchosen numbering on the other hand the clusterability of a class do not depend on the decision which numbering of the class be use as a hypothesis space for the clusterer these idea be demonstrate in the context of geometrically define class clustering of many of these class require besides the text for the clustering task some additional information the class of convex hull of finitely many point in a rational vector space can be cluster with the number of cluster as additional information interestingly the class of polygon lrb together with they interior rrb be clusterable if the number of cluster and the overall number of vertex of these cluster be give to the clusterer as additional information intriguingly this additional information be not sufficient for class include figure with hole while some class be unclusterable due to they topological structure other be only computationally intractable oracle can be use to distinguish between both case the former can not be cluster use any oracle while the latter can be cluster use some oracle it be show that there be maximal oracle that allow clustering of all problem that can be cluster with the help of some oracle in particular e be maximal iff e t k e doi 101137 050629112 hypothesis space clusterer disjoint number of cluster clusterability	Electronic_Colloquium_on_Computational_Complexity_(ECCC)	
881953	sanjay_jain	the intrinsic complexity of learning a survey	the theory of learn in the limit have be a focus of study by several researcher over the last three decade there have be several suggestion on how to measure the complexity or hardness of learning in this paper we survey the work do in one specific such measure call intrinsic complexity of learning we will be mostly concentrate on learn language with only a brief look at function learning intrinsic complexity fin function learning single grammar	Fundam._Inform.	School_of_Computing National_University_of_Singapore Singapore_119260 Republic_of_Singapore
882968	sanjay_jain frank_stephan ye_nan	prescribe learning of indexed family	this work extend study of angluin lange and zeugmann on how learnability of a language class depend on the hypothesis space use by the learner while previous study mainly focus on the case where the learner choose a particular hypothesis space the goal of this work be to investigate the case where the learner have to cope with all possible hypothesis space in that sense the present work combine the approach of angluin lange and zeugmann with the question of how a learner can be synthesize the investigation for the case of uniformly re class have be do by jain stephan and ye lsb 6 rsb this paper investigate the case for index family and give a special attention to the notion of conservative and non ushaped learning hypothesis space uniform conservative learner m iff	Fundam._Inform.	School_of_Computing National_University_of_Singapore Singapore_117590 Republic_of_Singapore._E-mails:_sanjay@comp.nus.edu.sg;_g0701171@nus.edu.sg
887112	ganesh_baliga john_case sanjay_jain	the synthesis of language learner	a index for a re class of language lrb by definition rrb be a procedure which generate a sequence of grammar define the class a index for a index family of language lrb by definition rrb be a procedure which generate a sequence of decision procedure define the family study be the metaproblem of synthesize from index for re class and for index family of language various kind of languagelearner for the corresponding class or family index many positive result as well as some negative result be present regard the existence of such synthesizer the negative result essentially provide lower bound for the positive result the proof of some of the positive result yield as pleasant corollary subsetprinciple or telltale style characterization for the learnability of the corresponding class or family index for example the index family of recursive language that can be behaviorally correctly identify from positive datum be surprisingly characterize by angluin s lrb 1980b rrb condition 2 lrb the subset principle for circumvent overgeneralization rrb doi 101006 inco 19982782 positive result negative result re class	Inf._Comput.	
887399	john_case sanjay_jain steffen_lange thomas_zeugmann	incremental concept learning for bounded data mining	important refinement of concept learning in the limit from positive datum considerably restrict the accessibility of input datum be study let c be any concept every infinite sequence of element exhausting c be call positive presentation of c in all learn model consider the learning machine compute a sequence of hypothesis about the target concept from a positive presentation of it with iterative learning the learn machine in make a conjecture have access to its previous conjecture and the latest datum item come in in kbounded examplememory inference lrb k be a priori fix rrb the learner be allow to access in make a conjecture its previous hypothesis its memory of up to k datum item it have already see and the next element come in in the case of kfeedback identification the learn machine in make a conjecture have access to its previous conjecture the latest datum item come in and on the basis of this information it can compute k item and query the database of previous datum to find out for each of the k item whether or not it be in the database lrb k be again a priori fix rrb in all case the sequence of conjecture have to converge to a hypothesis correctly describe the target concept we result be manyfold a infinite hierarchy of more and more powerful feedback learner in dependence on the number k of query allow to be ask be establish however the hierarchy collapse to 1feedback inference if only index family of infinite concept be consider and moreover its learning power be then equal to learn in the limit but it remain infinite for concept class of only infinite re concept both kfeedback inference and kbounded examplememory identification be more powerful than iterative learning but incomparable to one another furthermore there be case where redundancy in the hypothesis space be show to be a resource increase the learn power of iterative learner finally the union of at most k pattern language be show to be iteratively inferable doi 101006 inco 19982784 incremental learning data mining data item kdd recent	Inf._Comput.	
887405	john_case sanjay_jain arun_sharma	complexity issues for vacillatory function identification	it be previously show by barzdin and podnieks that one do not increase the power of learn program for function by allow learn algorithm to converge to a finite set of correct program instead of require they to converge to a single correct program in this paper we define some new subtle but natural concept of mind change complexity for function learning and show that if one bound this complexity for learn algorithm then by contrast with barzdin and podnieks result there be interesting and sometimes complicated tradeoff between these complexity bound bound on the number of final correct program and learn power doi 101006 inco 19951013 finite set mind change function learning cardinality barzdin	Inf._Comput.	
887407	john_case sanjay_jain arun_sharma	machine induction without revolutionary change in hypothesis size	this paper provide a begin study of the effect on inductive inference of paradigm shift whose absence be approximately model by various formal approach to forbid large change in the size of program conjecture one approach call severely parsimonious require all the program conjecture on the way to success to be nearly lrb ie within a recursive function of rrb minimal size it be show that this very conservative constraint allow learn infinite class of function but not infinite re class of function another approach call nonrevolutionary require all conjecture to be nearly the same size as one another this quite conservative constraint be nonetheless show to permit learn some infinite re class of function allow up to one extra bound size mind change towards a final program learn certainly do not appear revolutionary however somewhat surprisingly for scientific lrb inductive rrb inference it be show that there be class learnable with the nonrevolutionary constraint lrb respectively with severe parsimony rrb up to lrb i 1 rrb mind change and no anomaly which class can not be learn with no size constraint a unbounded finite number of anomaly in the final program but with no more than i mind change hence in some case the possibility of one extra mind change be considerably more liberating than removal of very conservative size shift constraint the proof of these result be also combinatorially interesting doi 101006 inco 19960064 revolutionary inductive inference anomaly paradigm shift computable function	Inf._Comput.	
887908	mark_a._fulk sanjay_jain	approximate inference and scientific method	a new identification criterion motivate by notion of successively improve approximation in the philosophy of science be define it be show that the class of recursive function be identifiable under this criterion this result be extend to permit somewhat more realistic type of datum than usual this criterion be then modify to consider restriction on the quality of approximation and the new criterion be compare to exist criterion doi 101006 inco 19941084	Inf._Comput.	
888352	sanjay_jain	robust behaviorally correct learning		Inf._Comput.	
888353	sanjay_jain efim_b._kinber	learning language from positive datum and a finite number of query	a computational model for learn language in the limit from full positive datum and a bounded number of query to the teacher lrb oracle rrb be introduce and explore equivalence superset and subset query be consider lrb for the latter one we consider also a variant when the learner test every conjecture but the number of negative answer be uniformly bound rrb if the answer be negative the teacher may provide a counterexample we consider several type of counterexample arbitrary least counterexample the one whose size be bound by the size of positive datum see so far and no counterexample a number of hierarchy base on the number of query lrb answer rrb and type of answerscounterexamples be establish capability of learn with different type of query be compare in most case one or two query of one type can sometimes do more than any bounded number of query of another type still surprisingly a finite number of subset query be sufficient to simulate the same number of equivalence query when behaviourally correct learner do not receive counterexample and may have unbounded number of error in almost all conjecture doi 101007 978354030538530 input language negative counterexample bounded number superset positive datum	Inf._Comput.	School_of_Computing National_University_of_Singapore Singapore Singapore
888354	sanjay_jain efim_b._kinber christophe_papazian carl_h._smith rolf_wiehagen	on the intrinsic complexity of learn recursive function	the intrinsic complexity of learn compare the difficulty of learn class of object by use some reducibility notion for several type of learn recursive function both natural complete class be exhibit and necessary and sufficient condition for completeness be derive informally a class be complete iff both its topological structure be highly complex while its algorithmic structure be easy some selfdescribing class turn out to be complete furthermore the structure of the intrinsic complexity be show to be much richer than the structure of the mind change complexity though in general intrinsic complexity and mind change complexity can behave orthogonally 1 introduction the problem of learn infinite object from grow finite sample of they behavior have attract much attention in recent decade in inductive inference the object to be learn be recursive function ie computable function be everywhere define on the set n of natural number the finite sample give to the learning machine be just initial segment of the infinite sequence of all the value of the corresponding function the machine be say to learn that function if when feed increase initial segment it eventually produce a program of the corresponding function and never change its mind thereafter a machine learn a class of function if it learn every function from that class this be basically the concept of learn in the limit introduce in lsb gol67 rsb other criterion for learn have be study doi 101145 307400307465 intrinsic complexity re complexity of learning subclass complete class	Inf._Comput.	School_of_Computing National_University_of_Singapore 119260_Singapore
888355	sanjay_jain efim_b._kinber rolf_wiehagen	learn all subfunction of a function	sublearning a model for learning of subconcept of a concept be present sublearning a class of total recursive function informally mean to learn all function from that class together with all of they subfunction while in language learn it be know to be impossible to learn any infinite language together with all of its sublanguage the situation change for sublearning of function several type of sublearning be define and compare to each other as well as to other learn type for example in some case sublearn coincide with robust learning furthermore whereas in usual function learn there be class that can not be learn consistently all sublearnable class of some natural type can be learn consistently moreover the power of sublearning be characterize in several term thereby establish a close connection to measurable class and variant of this notion as a consequence there be rich class which do not need any selfreferential code for sublearn they doi 101016 jic 200403003 re subfunction learnable finite card	Inf._Comput.	School_of_Computing National_University_of_Singapore 3_Science_Drive_2 Singapore_117543 Singapore
888356	sanjay_jain wolfram_menzel frank_stephan	class with easily learnable subclass	in this paper we study the question of whether identifiable class have subclass which be identifiable under a more restrictive criterion the choose framework be inductive inference in particular the criterion of explanatory learning lrb ex rrb of recursive function as introduce by gold in 1967 among the more restrictive criterion be finite learning where the learner output on every function to be learn exactly one hypothesis lrb which have to be correct rrb the topic of the present paper be the natural variant lrb a rrb and lrb b rrb below of the classical question whether a give learning criterion like finite learning be more restrictive than exlearning lrb a rrb do every infinite exidentifiable class have a infinite finitely identifiable subclass lrb b rrb if a infinite exidentifiable class s have a infinite finitely identifiable subclass do it necessarily follow that some appropriate learner exidentify s as well as finitely identify a infinite subclass of s these question be also treat in the context of ordinal mind change bound doi 101007 354036169319 finite learning infinite mind change subclasses class s	Inf._Comput.	School_of_Computing National_University_of_Singapore Singapore_119260 Singapore
888357	sanjay_jain frank_stephan	learning by switch type of information	the present work be dedicate to the study of mode of datapresentation in the range between text and informant within the framework of inductive inference in this study the learner alternatingly request sequence of positive and negative datum we define various formalization of valid datum presentation in such a scenario we resolve the relationship between these different formalization and show that one of these be equivalent to learn from informant we also show a hierarchy form lrb for each of the formalization study rrb by consider the number of switch between request for positive and negative datum doi 101007 354045583317 negative datum formalization learnable iff informant	Inf._Comput.	School_of_Computing National_University_of_Singapore 3_Science_Drive_2 Singapore_117543 Singapore
888358	sanjay_jain arun_sharma	learning in the presence of partial explanation	1 the effect of a partial explanation as additional information in the learning process be investigate a scientist perform experiment to gather experimental datum about some phenomenon and then try to construct a explanation lrb or theory rrb for the phenomenon a plausible model for the practice of science be a inductive inference machine lrb scientist rrb learn a program lrb explanation rrb from graph lrb set of experiment rrb of a recursive function lrb phenomenon rrb it be argue that this model of science be not a adequate one as scientist in addition to perform experiment make use of some approximate partial explanation base on the state of the art knowledge about that phenomenon a attempt have be make to model this partial explanation as a additional information in the scientific process it be show that inference capability of machine be improve in the presence of such a partial explanation the quality of this additional information be modeled use certain density notion it be show that additional information about a better quality partial explanation enhance the inference capability of learn machine as scientist more than a not so good partial explanation similar enhancement to inference of approximation a more sophisticated model of science be demonstrate inadequacy in gold s paradigm of language learning be investigate it be argue that gold s model fail to incorporate certain additional information that child get from they environment child be sometimes tell about some grammatical rule that enumerate element of the language it be argue that these rule be a kind of additional information they enable child to see in advance element that be yet to appear in they environment also child be be give some information about what be not in the language sometimes they be rebuke for make incorrect utterance or be tell of a rule that enumerate certain nonelement of the language a attempt have be make to extend gold s model to incorporate both the above type of additional information it be show that either type of additional information 2 enhance the learn capability of formal language learn device doi 101016 08905401 lrb 91 rrb 900432 language learning learning capability learning machine partial explanation grammar	Inf._Comput.	
888359	sanjay_jain arun_sharma	learn with the knowledge of a upper bind on program size	two learn situation be consider machine identification of program from graph of recursive function lrb modeling inductive hypothesis formation rrb and machine identification of grammar from text of recursively enumerable language lrb modeling first language acquisition rrb both these learn model be extend to account for situation in which a learn machine be provide additional information in the form of knowledge about a upperbound on the minimal size program lrb grammar rrb for the function lrb language rrb be identify for a number of such extension it be show that larger class of function lrb language rrb can be algorithmically identify in the presence of upperbound information numerous interesting relationship be show between different model of learning number of anomaly allow in the infer program lrb grammar rrb and number of anomaly allow in the upperbound information doi 101006 inco 19931005 machine identification upperbound learning model anomaly grammar	Inf._Comput.	
888360	sanjay_jain arun_sharma	computational limit on team identification of languages	a team of learn machine be a multiset of learn machine a team be say to successfully identify a concept just in case each member of some nonempty subset of predetermined size of the team identify the concept team identification of program for computable function from they graph have be investigate by smith pitt show that this notion be essentially equivalent to function identification by a single probabilistic machine the present paper introduce motivate and study the more difficult subject of team identification of grammar for language from positive datum it be show that a analog of pitt s result about equivalence of team function identification and probabilistic function identification do not hold for language identification and the result in the present paper reveal a very complex structure for team language identification it be also show that for certain case probabilistic language identification be strictly more powerful than team language identification proof of many result in the present paper involve very sophisticated diagonalization argument two very general tool be present that yield proof of new result from simple arithmetic manipulation of the parameter of known one doi 101006 inco 19960081 analog language identification team identification diagonalization function identification	Inf._Comput.	
888361	sanjay_jain arun_sharma	elementary formal dystem intrinsic complexity and procrastination		Inf._Comput.	
888362	sanjay_jain frank_stephan sebastiaan_terwijn	count extensional difference in bclearning	let bc be the model of behaviourally correct function learn as introduce by b arzdin lsb 4 rsb and case and smith lsb 8 rsb we introduce a mind change hierarchy for bc count the number of extensional difference in the hypothesis of a learner we compare the result model bc n to model from the literature and discuss confidence team learning and finitely defective hypothesis among other thing we prove that there be a tradeoff between the number of semantic mind change and the number of anomaly in the hypothesis we also discuss consequence for language learning in particular we show that in contrast to the case of function learning the family of class that be confidently bclearnable from text be not close under finite union doi 101007 978354045257721 function learning	Inf._Comput.	
888363	sanjay_jain arun_sharma mahendran_velauthapillai	finite identification of function by team with success ratio 1 over2 and above	consider a scenario in which a algorithmic machine m be be feed the graph of a computable function f m be say to finitely identify f just in case after inspect a finite portion of the graph of f it emit its first conjecture which be a program for f and it never abandon this conjecture thereafter a team of machine be a multiset of such machine a team be say to be successful just in case each member of some nonempty subset of predetermined size of the team be successful the ratio of the number of machine require to be successful to the size of the team be refer to as the success ratio of the team the present paper investigate the finite identification of computable function by team of learn machine the result present complete the picture for team with success ratio 1 2 and greater it be show that at success ratio 1 2 introduce redundancy in the team can result in increase learn power in particular it be establish that larger collection of function can be learn by employ team of 4 machine and require at least 2 to be successful than by employ team of 2 machine and require at least 1 to be successful surprisingly it be also show that introduce further redundancy at success ratio 1 2 do not yield any extra learning power in particular it be show that the collection of function that can be finitely identify by a team of 2m machine require at least m to be successful be the same as the collection of function that can be finitely identify by a team of 4 machine require at least 2 to be successful if m be even and the collection of function that can be identify by a team of 2 machine require at least 1 to be successful if m be odd these latter result require development of sophisticated simulation technique doi 101006 inco 19951133	Inf._Comput.	
889905	lorenzo_carlucci sanjay_jain efim_b._kinber frank_stephan	variation on ushaped learning	foreword this technical report contain a research paper development or tutorial article which have be submit for publication in a journal or for consideration by the commission organization the report represent the idea of its author and should not be take as the official view of the school or the university any discussion of the content of the report should be send to the author at the address show on the cover abstract the paper deal with the follow problem be return to wrong conjecture necessary to achieve full power of learning return to wrong conjecture complement the paradigm of ushaped learning lsb 3792327 rsb when a learner return to old correct conjecture we explore we problem for classical model of learn in the limit txtexlearning when a learner stabilize on a correct conjecture and txtbclearning when a learner stabilize on a sequence of grammar represent the target concept in both case we show that surprisingly return to wrong conjecture be necessary to achieve full power of learning on the other hand it be neither necessary to resort to a invertedushaped behaviour lrb a wrongcorrectwrong pattern rrb nor to return to old overgeneralize conjecture contain element not belong to the target language we also consider we problem in the context of socalled vacillatory learning when a learner stabilize to a finite number of correct grammar in this case we show that return to old wrong conjecture use a invertedushaped strategy as well as return to old overgeneralize conjecture be necessary for full learn power we also show that surprisingly learner consistent with the input see so far can be make decisive lsb 324 rsb they do not have to return to any old conjecture wrong or right	Inf._Comput.	School_of_Computing National_University_of_Singapore Singapore Republic_of_Singapore
889986	sanjay_jain frank_stephan	learning in friedberg numbering	in this paper we consider learnability in some special numbering such as friedberg numbering which contain all the recursively enumerable language but have simpler grammar equivalence problem compare to acceptable numbering we show that every explanatorily learnable class can be learn in some friedberg numbering however such a result do not hold for behaviourally correct learning or finite learning one can also show that some friedberg numbering be so restrictive that all class which can be explanatorily learn in such friedberg numbering have only finitely many infinite language we also study similar question for several property of learner such as consistency conservativeness prudence iterativeness and non ushaped learning besides friedberg numbering we also consider the above problem for programming system with krecursive grammar equivalence problem doi 101016 jic 200803001 txtex learnability friedberg numbering learnable class grammar	Inf._Comput.	Department_of_Computer_Science National_University_of_Singapore Singapore_117590 Republic_of_Singapore
890008	lorenzo_carlucci john_case sanjay_jain frank_stephan	result on memorylimited ushaped learning	ushaped learning be a learn behaviour in which the learner first learn a give target behaviour then unlearn it and finally relearn it such a behaviour observe by psychologist for example in the learning of pasttense of english verb have be widely discuss among psychologist and cognitive scientist as a fundamental example of the nonmonotonicity of learning previous theory literature have study whether or not ushaped learning in the context of gold s formal model of learn language from positive datum be necessary for learn some task it be clear that human learning involve memory limitation in the present paper we consider then the question of the necessity of ushaped learning for some learning model feature memory limitation we result show that the question of the necessity of ushaped learning in this memorylimited setting depend on delicate tradeoff between the learner s ability to remember its own previous conjecture to store some value in its longterm memory to make query about whether or not item occur in previously see datum and on the learner s choice of hypothesis space doi 101016 jic 200704001 memory limitation longterm memory psychologist ushape previous conjecture	Inf._Comput.	School_of_Computing National_University_of_Singapore Singapore_117543 Republic_of_Singapore
890011	sanjay_jain steffen_lange sandra_zilles	some natural condition on incremental learning	the present study aim at insight into the nature of incremental learning in the context of gold s model of identification in the limit with a focus on natural requirement such as consistency and conservativeness incremental learning be analyse both for learn from positive example and for learn from positive and negative example the result obtain illustrate in which way different consistency and conservativeness demand can affect the capability of incremental learner these result may serve as a first step towards characterise the structure of typical class learnable incrementally and thus towards elaborate uniform incremental learning method doi 101016 jic 200706002 incremental learning conservativeness	Inf._Comput.	School_of_Computing National_University_of_Singapore Singapore_117590 Singapore
890012	sanjay_jain efim_b._kinber	iterative learning from positive datum and negative counterexample	a model for learn in the limit be define where a lrb socalled iterative rrb learner get all positive example from the target language test every new conjecture with a teacher lrb oracle rrb if it be a subset of the target language lrb and if it be not then it receive a negative counterexample rrb and use only limited longterm memory lrb incorporate in conjecture rrb three variant of this model be compare when a learner receive least negative counterexample the one whose size be bound by the maximum size of input see so far and arbitrary one a surprising result be that sometimes absence of bounded counterexample can help a iterative learner whereas arbitrary counterexample be useless we also compare we learnability model with other relevant model of learnability in the limit study how we model work for index class of recursive language and show that learner in we model can work in nonushaped way never abandon the first right conjecture doi 101007 1189484115 negative datum longterm memory ncit learnability counterexample	Inf._Comput.	School_of_Computing National_University_of_Singapore Singapore_117590 Singapore
908962	john_case sanjay_jain arun_sharma	on learning limit program	machine learning of italic limit program italic lrb ie program allow finitely many mind change about they legitimate output rrb for italic computable italic function be study learning of italic iterated italic limit program be also study to partially motivate these study it be show that in some case interesting global property of computable function can be prove from suitable lrb italic n italic 1 rrb iterate limit program for they which can italic not italic be prove from italic any n italic iterated limit program for they it be show that learn power be increase when lrb italic n italic 1 rrb iterate limit program rather than italic n italic iterated limit program be to be learn many tradeoff result be obtain regard learn power number lrb possibly zero rrb of limit take program size constraint and number of error tolerate in final program learn doi 101145 130385130407 computable function	Int._J._Found._Comput._Sci.	
909135	lane_a._hemachandra sanjay_jain	on the limitations of locally robust positive reduction	polynomialtime positive reduction as introduce by selman be by definition globally robust they be positive with respect to all oracle this paper study the extent to which the theory of positive reduction remain intact when they global robustness assumption be remove we note that twosided locally robust positive reduction reduction that be positive with respect to the oracle to which the reduction be make be sufficient to retain all crucial property of globally robust positive reduction in contrast we prove absolute and relativized result show that onesided local robustness fail to preserve fundamental property of positive reduction such as the downward closure of np doi 101007 354052048144 reducibility global robustness equality positive reduction oracle	Int._J._Found._Comput._Sci.	
909146	lane_a._hemaspaandra sanjay_jain nikolai_k._vereshchagin	banish robust turing completeness	this paper prove that promise class be so fragilely structure that they do not robustly lrb ie with respect to all oracle rrb possess turinghard set even in class far larger than themselves in particular this paper show that fewp do not robustly possess turing hard set for up coup and ip coip do not robustly possess turing hard set for zpp and ip coip do not robustly possess turing complete set this both resolve open question of whether promise class lack robust downward closure under ture reduction lrb eg r up fewp rrb might robustly have ture complete set and extend the range of class know not to robustly contain manyone complete set doi 101007 bfb0023873 complete set fewp promise class	Int._J._Found._Comput._Sci.	
909211	sanjay_jain	a infinite class of functions identifiable use minimal program in all kolmogorov numberings	identification of program for computable function from they graph by algorithmic device be a well study problem in learn theory freivalds and chen consider identification of minimal and nearly minimal program for function from they graph freivalds show that there exist a gdel numbering in which only finite class of function can be identify use minimal program to address such problem freivalds later consider minimal identification in kolmogorov numberings kolmogorov numbering be in some sense optimal numbering and have some nice property freivalds show that for every kolmogorov numbering there exist a infinite class of function which can be identify use minimal program note that these infinite class of function may depend on the kolmogorov numbering it be leave open whether there exist a infinite class of function c such that c can be identify use minimal program in every kolmogorov numbering we show the existence of such a class doi 101142 s012905419500007x numbering class of function minimal natural number infinite class	Int._J._Found._Comput._Sci.	
909212	sanjay_jain	minimal concept identification and reliability	identification by algorithmic device of grammar for language from positive datum be a well study problem in this paper we be mainly concern about the learnability of index family of uniformly recursive language mukouchi introduce the notion of minimal and reliable minimal concept inference from positive datum he leave open a question about whether every index family of uniformly recursive language that be minimally inferable be also reliably minimally inferable we show that this be not the case doi 101142 s0129054198000209 positive datum recursive languages	Int._J._Found._Comput._Sci.	
931927	sanjay_jain	on a question about learning nearly minimal program	identification of program for computable function from they graph by algorithmic device be a well study problem in learn theory freivalds and chen consider identification of minimal and nearly minimal program for function from they graph the present paper solve the follow question leave open by chen be it the case that for any collection of computable function c such that some machine can finitely learn a nearly minimal lrb n 1 rrb error program for every function in c there exist another machine that can learn in the limit a nerror program lrb which need not be nearly minimal rrb for every function in c we answer this question negatively doi 101016 00200190 lrb 94 rrb 00172u mind change natural number decoration max m error	Inf._Process._Lett.	
931928	sanjay_jain	on a question of nearly minimal identification of function	suppose a and b be class of recursive function a be say to be a mcover lrb cover rrb for b iff for each g b there exsit a f a such that f differ from g on at most m input lrb finitely many input rrb c a class of recursive function be aimmune iff c be infinite and every recursively enumerable subclass of c have a finite acover c be aisolated iff c be finite or aimmune chen lsb che81 rsb conjecture that every class of recursive function that be mex midentifiable be isolate we refute this conjecture doi 101016 s00200190 lrb 99 rrb 000514 cover learning machine iff recursive function	Inf._Process._Lett.	
931929	sanjay_jain arun_sharma	on the nonexistence of maximal inference degrees for language identification	identification of grammar lrb r e index rrb for recursively enumerable language from positive datum by algorithmic device be a well study problem in learn theory the present paper consider identification of r e language by machine that have access to membership oracle for noncomputable set it be show that for any set a there exist another set b such that the collection of r e language that can be identify by machine with access to a membership oracle for b be strictly larger than the collection of r e language that can be identify by machine with access to a membership oracle for a in other word there be no maximal inference degree for language identification doi 101016 00200190 lrb 93 rrb 902293 language identification enumerable language positive datum oracle grammar	Inf._Process._Lett.	
935214	sanjay_jain	on some open problem in reflective inductive inference	in this paper we show that there exist class of function which can be learn by a finite learner which reflect on its capability but not learnable by a consistent learner which optimistically reflect on its capability this solve the two mention open problem from lsb gri08 rsb doi 101016 jipl 200810006 class be input data class of function open problem consistent learner	Inf._Process._Lett.	School_of_Computing National_University_of_Singapore Singapore_117590
970373	subhash_c._sarin balaji_nagarajan sanjay_jain lingrui_liao	analytic evaluation of the expectation and variance of different performance measure of a schedule on a single machine under processing time variability		J._Comb._Optim.	
971443	ganesh_baliga john_case sanjay_jain	language learning with some negative information	gold style language learning be a formal theory of learn from example by algorithmic device call learn machine originally motivate by child language learning it feature the algorithmic synthesis lrb in the limit rrb of grammar for formal language from information about those language in traditional gold style language learning learn machine be not provide with negative information ie information about the complement of the input language we investigate two approach to provide small amount of negative information and demonstrate in each case a strong result increase in learn power finally we show that small packet of negative information also lead to increase speed of learning this result agree with a psycholinguistic hypothesis of mcneill correlate the availability of parental expansion with the speed of child language development doi 101006 jcss 19951066 txtex txtbc learning power positive information negative information	J._Comput._Syst._Sci.	
971658	john_case sanjay_jain	synthesize learners tolerating computable noisy data	a index for a re class of language lrb by deenition rrb generate a sequence of grammar deen the class a index for a index family of language lrb by deenition rrb generate a sequence of decision procedure deen the family f stephan s model of noisy datum be employ in which roughly correct datum crop up innnitely often and incorrect datum only nitely often in a completely computable universe all datum sequence even noisy one be computable new to the present paper be the restriction that noisy datum sequence be nonetheless computable study then be the synthesis from index for re class and for index family of language of various kind of noisetolerant languagelearner for the corresponding class or family index where the noisy input datum sequence be restricted to be computable many positive result as well as some negative result be present regard the existence of such synthesizer the main positive result be surprisingly more positive than its analog in the case the noisy datum be not restricted to be computable grammar for each index family can be learn behaviorally correctly from computable noisy positive datum the proof of another positive synthesis result yield as a pleasant corollary a strict subsetprinciple or telltale style characterization for the computable noisetolerant behaviorally correct learnability of grammar from positive and negative datum of the corresponding family index doi 101007 354049730716 decision procedure deenition noisy data positive datum grammar	J._Comput._Syst._Sci.	
971659	john_case sanjay_jain franco_montagna giulia_simi andrea_sorbi	on learn to coordinate random bit help insightful normal form and competency isomorphism	a mere bounded number of random bit judiciously employ by a probabilistically correct algorithmic coordinator be show to increase the power of learn to coordinate compare to deterministic algorithmic coordinator furthermore these probabilistic algorithmic coordinator be provably not characterize in power by team of deterministic one a insightful enumeration technique base normal form characterization of the class that be learnable by total computable coordinator be give these normal form be for insight only since it be show that the complexity of the normal form of a total computable coordinator can be infeasible compare to the original coordinator montagna and osherson show that the competence class of a total coordinator can not be strictly improve by another total coordinator it be show in the present paper that the competency of any two total coordinator be the same modulo isomorphism furthermore a completely effective index set version of this competency isomorphism result be give where all the coordinator be total computable we also investigate the competence class of total coordinator from the point of view of topology and descriptive set theory doi 101007 978354045167951 coordinator normal form	J._Comput._Syst._Sci.	Department_of_Computer_Science School_of_Computing National_University_of_Singapore Singapore_117543 Republic_of_Singapore
971660	john_case sanjay_jain matthias_ott arun_sharma frank_stephan	robust learning aid by context	empirical study of multitask learning provide some evidence that the performance of a learning system on its intended target improve by present to the learning system related task also call context as additional input angluin gasarch and smith as well as kinber smith velauthapillai and wiehagen have provide mathematical justiication for this phenomenon in the inductive inference framework however they proof rely heavily on selfreferential code trick that be they directly code the solution of the learning problem into the context fulk have show that for the exand bcanomaly hierarchy such result which rely on selfreferential code trick may not hold robustly in this work we analyze robust version of learn aid by context and show that in contrast to fulk s result above the robust version of y support by the deutsche forschungsgemeinschaft lrb dfg rrb graduiertenkolleg beherrschbarkeit komplexer systeme lrb grk 2092 96 rrb these learn notion be still very powerful also study be the diiculty of the functional dependence between the intended target task and useful associate context doi 101145 279943279952 robust version	J._Comput._Syst._Sci.	
971661	john_case sanjay_jain arun_sharma	vacillatory learning of nearly minimal size grammar	in gold s influential language learn paradigm a learning machine converge in the limit to one correct grammar in a attempt to generalize gold s paradigm case consider the question whether people might converge to vacillate between up to lrb some integer rrb n 1 distinct but equivalent correct grammar he show that larger class of language can be algorithmically learn lrb in the limit rrb by converge to up to n 1 rather than up to n correct grammar he also argue that for small n 1 it be plausible that people might sometimes converge to vacillate between up to n grammar the insistence on small n be motivate by the consideration that for large n at least one of n grammar would be too large to fit in people head of course even for gold s n 1 case the single grammar converge to in the limit may be infeasibly large a interesting complexity restriction to make then on the final grammar lrb s rrb converge to in the limit be that they all have small size in this paper we study some of the tradeoff in learn power involve in make a welldefined version of this restriction we show and exploit as a tool the desirable property that the learn power under we sizerestricted criterion lrb for successful learning rrb be independent of the underlie acceptable programming system we characterize the power of we sizerestricted criterion and use this characterization to prove that some class of language which can be learn by converge in the limit to up to n 1 nearly minimal size correct grammar can not be learn by converge to up to n unrestricted grammar even if these latter grammar be allow to have a finite number of anomaly lrb ie mistake rrb per grammar we also show that there be no loss of learn power in demand that the final grammar be nearly minimal size iff one be willing to tolerate a unbounded finite number of anomaly in the final grammar and there be a constant bind on the number of different grammar converge to in the limit hence if we allow a unbounded finite number of anomaly in the final grammar and the number of different grammar converge to in the limit be unbounded but finite lrb or if there be a constant bind on the number of anomaly allow in the final grammar rrb then there be a loss doi 101016 s00220000 lrb 05 rrb 800467 language learning learning power anomaly finite number grammar	J._Comput._Syst._Sci.	
971662	john_case sanjay_jain frank_stephan rolf_wiehagen	robust learning rich and poor	a class c of recursive function be call robustly learnable in the sense i lrb where i be any success criterion of learning rrb if not only c itself but even all transform class lrb c rrb where be any general recursive operator be learnable in the sense i it be already show before see lsb ful90 jsw01 rsb that for i ex lrb learn in the limit rrb robust learning be rich in that there be class be both not contain in any recursively enumerable class of recursive function and nevertheless robustly learnable for several criterion i the present paper make much more precise where we can hope for robustly learnable class and where we can not this be achieve in two way first for i ex it be show that only consistently learnable class can be uniformly robustly learnable second some other learning type i be classify as to whether or not they contain rich robustly learnable class moreover the first result on separate robust learning from uniformly robust learning be derive doi 101016 jjcss 200310005 deal robust learning recursive function enumerable learnable class	J._Comput._Syst._Sci.	
971971	mark_a._fulk sanjay_jain daniel_n._osherson	open problem in systems that learn	in this paper we solve some of the open problem in lsb 19 rsb we also give partial solution to some other open problem in the book in particular we show that the collection of class of language that can be identify on noisy text lrb ie a text which may contain some element which be not in the language be learn rrb strictly contain the collection of class of language that can be identify on imperfect text lrb ie a text which may contain some extra element and may leave out some element from the language be learn rrb we also show that memory limited identification be strictly more restrictive that memory bound identification besides solve the above two open problem from lsb 19 rsb we also give partial solution to other open problem in lsb 19 rsb doi 101016 s00220000 lrb 05 rrb 800728 exm iim partial solution open problem machine m	J._Comput._Syst._Sci.	
972297	sanjay_jain	program synthesis in the presence of infinite number of inaccuracies	most study modeling inaccurate datum in gold style learn consider case in which the number of inaccuracy be finite the present paper argue that this approach be not reasonable for modeling inaccuracy in concept that be infinite in nature lrb for example graph of computable function rrb the effect of infinite number of inaccuracy in the input datum in gold s model of learning be consider in the context of identification in the limit of computer program from graph of computable function three kind of inaccuracy namely noisy datum incomplete datum and imperfect datum be consider the amount of each of these inaccuracy in the input be measure use certain density notion a number of interesting hierarchy result be show base on the density of inaccuracy present in the input datum several result establish tradeoff between the density and type of inaccuracy be also derive doi 101007 354058520675 computer program inaccuracy computable function finite number infinite number	J._Comput._Syst._Sci.	
972299	sanjay_jain	learn with refutation	in they pioneering work mukouchi and arikawa model a learning situation in which the learner be expect to refute text which be not representative of l the class of language be identify lange and watson extend this model to consider justified refutation in which the learner be expect to refute text only if it contain a finite sample unrepresentative of the class l both the above study be in the context of index family of recursive language we extend this study in two direction firstly we consider general class of recursively enumerable language secondly we allow the machine to either identify or refute the unrepresentative text lrb respectively text contain finite unrepresentative sample rrb we observe some surprising difference between we result and the result obtain for learn index family by lange and watson doi 101006 jcss 19981591 txtex class of languages input text refutation general class	J._Comput._Syst._Sci.	
972301	sanjay_jain efim_b._kinber	intrinsic complexity of learn geometrical concept from positive datum	intrinsic complexity be use to measure the complexity of learn area limit by brokenstraight line lrb call open semihull rrb and intersection of such area any strategy learn such geometrical concept can be view as a sequence of primitive basic strategy thus the length of such a sequence together with the complexity of the primitive strategy use can be regard as the complexity of learn the concept in question we obtain the best possible lower and upper bound on learn open semihull as well as match upper and lower bound on the complexity of learn intersection of such area surprisingly upper bound in both case turn out to be much lower than those provide by natural learning strategy another surprising result be that learn intersection of open semihull turn out to be easier than learn open semihull themselves doi 101016 s00220000 lrb 03 rrb 000679 intrinsic complexity surprising upper and lower bounds init geometrical concept	J._Comput._Syst._Sci.	Department_of_Computer_Science School_of_Computing National_University_of_Singapore Lower_Kent_Ridge_Road Singapore_119260 Singapore
972303	sanjay_jain efim_b._kinber rolf_wiehagen	language learning from texts degree of intrinsic complexity and they characterization	this paper deal with two problem 1 rrb what make language to be learnable in the limit by natural strategy of vary hardness 2 rrb what make class of language to be the hardest one to learn to quantify hardness of learning we use intrinsic complexity base on reduction between learning problem two type of reduction be consider weak reduction map text lrb representation of language rrb to text and strong reduction map language to language for both type of reduction characterization of complete lrb hardest rrb class in term of they algorithmic and topological potential have be obtain to characterize the strong complete degree we discover a new and natural complete class capable of coding any learning problem use density of the set of rational number we have also discover and characterize rich hierarchy of degree of complexity base on core natural learning problem the class in these hierarchy contain multidimensional language where the information learn from one dimension aid to learn other dimension in one formalization of this idea the grammar learn from the dimension 1 2 k specify the subspace for the dimension k 1 while the learning strategy for every dimension be predeen in we other formalization a pattern learn from the dimension k speciie the learning strategy for the dimension k 1 a number of open problem be discuss doi 101006 jcss 20011759 formalization init hardness class of languages single	J._Comput._Syst._Sci.	
972305	sanjay_jain arun_sharma	characterize language identification by standardize operations	notion from formal language learn theory be characterize in term of standardize operation on class of recursively enumerable language algorithmic identification in the limit of grammar from text presentation of recursively enumerable language be a central paradigm of language learning a mapping f from the set of all grammar into the set of all grammar be a standardize operation on a class of recursively enumerable language l just in case f map any grammar for any language l l to a canonical grammar for l investigate connection between these two notion be the subject of this paper doi 101016 s00220000 lrb 05 rrb 800881 language learning txtex language l enumerable language grammar	J._Comput._Syst._Sci.	
972307	sanjay_jain arun_sharma	the intrinsic complexity of language identification	a new investigation of the complexity of language identification be undertake use the notion of reduction from recursion theory and complexity theory the approach refer to as	J._Comput._Syst._Sci.	
972309	sanjay_jain carl_h._smith rolf_wiehagen	robust learning be rich	intuitively a class of object be robustly learnable if not only this class itself be learnable but all of its computable transformation remain learnable as well in that sense be learnable robustly seem to be a desirable property in all field of learning we will study this phenomenon within the paradigm of inductive inference here a class of recursive function be call robustly learnable under a success criterion i iff all of its image under general recursive operator be learnable under the criterion i fulk lsb ful90 rsb show the existence of a nontrivial class which be robustly learnable under the criterion ex however several of the hierarchy lrb such as the anomaly hierarchy for ex and bc rrb do not stand robustly hence up to now it be not clear if robust learning be really rich the main intention of this paper be to give strong evidence that	J._Comput._Syst._Sci.	
973188	sanjay_jain yen_kaow_ng tiong_seng_tay	learning language in a union	in inductive inference a machine be give word of a language lrb a recursively enumerable set in we setting rrb and the machine be say to identify the language if it correctly name the language in this paper we study identifiability of class of language where the union of up to a fixed number lrb n say rrb of language from the class be provide as input we distinguish between two different scenario in one scenario the learner need only to name the language which result from the union in the other the learner must individually name the language which make up the union lrb we say that the unioned language be discerningly identify rrb we define three kind of identification criterion base on this and by the use of some class of disjoint language demonstrate that the infer power of each of these identification criterion decrease as we increase the number of language allow in the union thus result in a infinite hierarchy for each identification criterion that be we show that for each n there exist a class of disjoint language where all union of up to n language from this class can be discerningly identify but there be no learner which identify every union of n 1 language from this class a comparison between the different identification criterion also yield similar hierarchy we give sufficient condition for class of language where the union can be discerningly identify and characterize such discerning learnability for the index family we then give naturally occur class of language that witness some of the earlier hierarchical result finally we present language class which be complete with respect to weak reduction lrb in term of intrinsic complexity rrb for we identification criterion doi 101016 jjcss 200601005 n language learnability identifiability discerning identification criterion	J._Comput._Syst._Sci.	School_of_Computing National_University_of_Singapore Singapore_119260 Singapore
973255	lorenzo_carlucci john_case sanjay_jain frank_stephan	nonushaped vacillatory and team learning	ushaped learning behaviour in cognitive development involve learning unlearn and relearn it occur for example in learn irregular verb the prior cognitive science literature be occupy with how human do it for example general rule versus table of exception this paper be mostly concern with whether ushaped learning behaviour may be necessary in the abstract mathematical setting of inductive inference that be in the computational learning theory follow the framework of gold all notion consider be learn from text that be from positive datum previous work show that ushaped learning behaviour be necessary for behaviourally correct learning but not for syntactically convergent learn in the limit lrb explanatory learning rrb the present paper establish the necessity for the hierarchy of class of vacillatory learning where a behaviourally correct learner have to satisfy the additional constraint that it vacillate in the limit between at most b grammar where b lcb 2 3 rcb non ushaped vacillatory learning be show to be restrictive every non ushaped vacillatorily learnable class be already learnable in the limit furthermore if vacillatory learning with the parameter b 2 be possible then non ushaped behaviourally correct learning be also possible but for b 3 surprisingly there be a class witness that this implication fail doi 101007 1156408920 learning behaviour txtex language l txtbc grammar	J._Comput._Syst._Sci.	School_of_Computing National_University_of_Singapore 3_Science_Drive_2 Singapore_117543 Republic_of_Singapore
973327	sanjay_jain efim_b._kinber	learning language from positive datum and negative counterexample	in this paper we introduce a paradigm for learn in the limit of potentially infinite language from all positive datum and negative counterexample provide in response to the conjecture make by the learner several variant of this paradigm be consider that reflect different conditionsconstraints on the type and size of negative counterexample and on the time for obtain they in particular we consider the model where 1 rrb a learner get the least negative counterexample 2 rrb the size of a negative counterexample must be bound by the size of the positive datum see so far 3 rrb a counterexample can be delay learn power limitation of these model relationship between they as well as they relationship with classical paradigm for learn language in the limit lrb without negative counterexample rrb be explore several surprising result be obtain in particular for gold s model of learn require a learner to syntactically stabilize on correct conjecture learner get negative counterexample immediately turn out to be as powerful as the one that do not get they for indefinitely lrb but finitely rrb long time lrb or be only tell that they latest conjecture be not a subset of the target language without any specific negative counterexample rrb another result show that for behaviourally correct learning lrb where semantic convergence be require from a learner rrb with negative counterexample a learner make just one error in almost all its conjecture have the ultimate power it can learn the class of all recursively enumerable language yet another result demonstrate that sometimes positive datum and negative counterexample provide by a teacher be not enough to compensate for full positive and negative datum doi 101016 jjcss 200706012 input language negative counterexample positive datum informant grammar	J._Comput._Syst._Sci.	School_of_Computing National_University_of_Singapore Singapore_117543 Singapore
980352	john_case keh-jiann_chen sanjay_jain	strong separation of learn class	suppose lc 1 and lc 2 be two machine learning class each base on a criterion of success suppose for every machine which learn a class of function accord to the lc 1 criterion of success there be a machine which learn this class accord to the lc 2 criterion in the case where the converse do not hold lc 1 be say to be separate from lc 2 it be show that for many such separated learning class from the literature a much stronger separation hold lrb c lc 1 rrb lrb c lrb lc 2 lc 1 rrb rrb lsb c c rsb it be also show that there be a pair of separated learning class from the literature for which the stronger separation just above do not hold a philosophical heuristic toward the design of artificially intelligent learning program be present with each strong separation result doi 101007 35405600419 correspond interpretation technical result est input and output criterion of success	J._Exp._Theor._Artif._Intell.	
980388	sanjay_jain	on a open problem in classification of language	smith wiehagen and zeugmann lrb 1997 rrb show a interesting connection between learn with bound number of mind change from informant and classification from informant they show that if a index family of language l be learnable via informant use at most m mind change then one can partition 2 can be classify from informant however smith wiehagen and zeugmann lrb 1997 rrb leave open whether a similar result also hold for learn from text we show that such a result do not hold for text doi 101080 09528130010029802 formal language mind change subclasses informant grammar	J._Exp._Theor._Artif._Intell.	
980390	sanjay_jain	strong monotonic and setdriven inductive inference	in a earlier paper kinber and stephan pose a open problem about whether every class of language which can be identify strong monotonically can also be identify by a setdriven machine we solve this question in this paper the answer to the question depend on whether the machine be require to be total or not the solution of this result uncover a finer gradation of the notion of setdrivenness doi 101080 095281397147275 formal definition formal language gradation answer to the question grammar	J._Exp._Theor._Artif._Intell.	
998211	ganesh_baliga john_case sanjay_jain mandayam_suraj	machine learning of higherorder programs	a generator program for a computable function lrb by definition rrb generate a infinite sequence of program all but finitely many of which compute that function machine learning of generator program for computable function be study to partially motivate these study it be show that in some case interesting global property for computable function can be prove from suitable generator program which can not be prove from any ordinary program for they the power lrb for variant of various learn criterion from the literature rrb of learn generator program be compare with the power of learn ordinary program the learn power in these case be also compare to that of learn limit program ie program allow finitely many mind change about they correct output doi 101007 bfb0023859 computable function generator program ordinary programs	J._Symb._Log.	
999575	sanjay_jain jochen_nessel	some independence result for control structure in complete numbering	acceptable programming system have many nice property like smntheorem composition and kleene recursion theorem those property be sometimes call control structure to emphasize that they yield tool to implement program in programming system it have be study among other by riccardi and royer how these control structure influence or even characterize the notion of acceptable programming system the following be a investigation how these control structure behave in the more general setting of complete numbering as define by mal cev and erov hypothesis space programming systems general setting numbering control structure	J._Symb._Log.	
999577	sanjay_jain arun_sharma	the structure of intrinsic complexity of learning	limit identification of re index for re language lrb from a presentation of element of the language rrb and limit identification of program for computable function lrb from a graph of the function rrb have serve as model for investigate the boundary of learnability recently a new approach to the study of intrinsic complexity of identification in the limit have be propose this approach instead of deal with the resource requirement of the learning algorithm use the notion of reducibility from recursion theory to compare and to capture the intuitive difficulty of learn various class of concept freivalds kinber and smith have study this approach for function identification and jain and sharma have study it for language identification the present paper explore the structure of these reducibility in the context of language identification it be show that there be a infinite hierarchy of language class that represent learn problem of increase difficulty it be also show that the language class in this hierarchy be incomparable under the reduction introduce to the collection of pattern language richness of the structure of intrinsic complexity be demonstrate by prove that any finite acyclic direct graph can be embed in the reducibility structure however it be also establish that this structure be not dense the question of embed any infinite acyclic direct graph be open doi 101007 3540591192176 fin infinite mind change finite weak	J._Symb._Log.	
1015648	sanjay_jain arun_sharma	team learning of computable languages	a team of learn machine be a multiset of learn machine a team be say to successfully learn a concept just in case each member of some nonempty subset of predetermined size of the team learn the concept team learning of language may be view as a suitable theoretical model for study computational limit on the use of multiple heuristic in learn from example team learning of recursively enumerable language have be study extensively however it may be argue that from a practical point of view all language of interest be computable this paper give theoretical result about team learnability of computable lrb recursive rrb language these result be mainly about two issue redundancy and aggregation the issue of redundancy deal with the impact of increase the size of a team and increase the number of machine require to be successful the issue of aggregation deal with condition under which a team may be replace by a single machine without any loss in learn ability the learning scenario consider be lrb a rrb identification in the limit of grammar for computable language lrb b rrb identification in the limit of decision procedure for computable language lrb c rrb identification in the limit of grammar for index family of computable language lrb d rrb identification in the limit of grammar for index family with a recursively enumerable class of grammar for the family as the hypothesis space scenario that can be model by team learning be also present doi 101007 s002249910003 decision procedure hypothesis space computable languages team learning grammar	Theory_Comput._Syst.	
1015649	sanjay_jain arun_sharma	prudence in vacillatory language identification	the present paper settle a question about prudent vacillatory identification of language consider a scenario in which a algorithmic device m be present with all and only the element of a language l and m conjecture a sequence possibly infinite of grammar three different criterion for success of m on l have be extensively investigate in formal language learn theory if m converge to a single correct grammar for l then the criterion of success be gold s seminal notion of txtexidentification if m converge to a finite number of correct grammar for l then the criterion of success be call txtfexidentification and if m after a finite number of incorrect guess output only correct grammar for l lrb possibly infinitely many distinct grammar rrb then the criterion of success be know as txtbcidentification a learn machine be say to be prudent accord to a particular criterion of success just in case the only grammar it ever conjecture be for language that it can learn accord to that criterion this notion be introduce by osherson stob and weinstein with a view to investigate certain proposal for characterize natural language in linguistic theory fulk show that prudence do not restrict txtexidentification and later kurtz and royer show that prudence do not restrict txtbcidentification the present paper show that prudence do not restrict txtfexidentification 1 introduction language be set of sentence and a sentence be a finite object the set of all possible sentence can be code into n the set of natural number hence language may be construe as subset of n a grammar for a language be a set of rule that accept lrb or equivalently generate lsb hu79 rsb rrb the language essentially any computer program may be view as a grammar language for which a grammar exist be call recursively enumerable henceforth we work under the assumption that natural language fall in the class of recursively enumerable language a text for a language l be any infinite sequence that list all and only the element of l repetition be permit motivate by psycholinguistic study which suggest that child be rarely if ever inform of grammatical error 1 gold lsb gol67 rsb introduce the seminal notion of identification in the limit lrb which we refer to as txtexidentification follow lsb cl82 rsb rrb as a model for first language acquisition accord to this paradigm a child lrb model as a machine rrb receive a text for a language l and simultaneously conjecture doi 101007 bf01303059 language learning language l prudence criterion of success grammar	Mathematical_Systems_Theory	
1058514	ganesh_baliga sanjay_jain arun_sharma	learning from multiple source of inaccurate data	most theoretical model of inductive inference make the idealized assumption that the datum available to a learner be from a single and accurate source the subject of inaccuracy in datum emanate from a single source have be address by several author the present paper argue in favor of a more realistic learning model in which datum emanate from multiple source some or all of which may be inaccurate three kind of inaccuracy be consider spurious datum lrb model as noisy text rrb miss datum lrb model as incomplete text rrb and a mixture of spurious and missing datum lrb model as imperfect text rrb motivate by the above argument the present paper introduce and theoretically analyze a number of inference criterion in which a learn machine be feed datum from multiple source some of which may be infect with inaccuracy the learning situation model be the identification in the limit of program from graph of computable function the main parameter of the investigation be kind of inaccuracy total number of datum source number of faulty datum source which produce datum within a acceptable bind and the bind on the number of error allow in the final hypothesis learn by the machine sufficient condition be determine under which for the same kind of inaccuracy for the same bind on the number of error in the final hypothesis and for the same bind on the number of inaccuracy learn from multiple text some of which may be inaccurate be equivalent to learn from a single inaccurate text the general problem of determine when learn from multiple inaccurate text be a restriction over learn from a single inaccurate text turn out to be combinatorially very complex significant partial result be provide for this problem several result be also provide about condition under which the detrimental effect of multiple text can be overcome by either allow more error in the final hypothesis or by reduce the number of inaccuracy in the text it be also show that the usual hierarchy result from allow extra error in the final program lrb result in increase learn power rrb and allow extra inaccuracy in the text lrb result in decrease learning power rrb hold finally it be demonstrate that in the context of learn from multiple inaccurate text spurious datum be better than miss datum which in turn be better than a mixture of spurious and missing datum 1 introduction a scenario in which a algorithmic learner doi 101007 35405600418 multiple text learning power inaccuracy spurious multiple source	SIAM_J._Comput.	
1058793	john_case sanjay_jain eric_martin arun_sharma frank_stephan	identify cluster from positive datum	the present work study clustering from a abstract point of view and investigate its property in the framework of inductive inference any class s consider be give by a numbering a 0 a 1 of nonempty subset of n or q k which be use as a hypothesis space a clustering task be a finite and nonempty set of index of pairwise disjoint set the class s be say to be clusterable if there be a algorithm which for every clustering task i converge in the limit on any text for i i a i to a finite set j of index of pairwise disjoint cluster such that j j a j i i a i a class be call semiclusterable if there be such a algorithm which find a j with the last condition relax to j j a j i i a i the relationship between natural topological property and clusterability be investigate topological property can provide sufficient or necessary condition for clusterability but they can not characterize it on one hand many interesting condition make use of both the topological structure of the class and a wellchosen numbering on the other hand the clusterability of a class do not depend on the decision which numbering of the class be use as a hypothesis space for the clusterer these idea be demonstrate in the context of geometrically define class clustering of many of these class require besides the text for the clustering task some additional information the class of convex hull of finitely many point in a rational vector space can be cluster with the number of cluster as additional information interestingly the class of polygon lrb together with they interior rrb be clusterable if the number of cluster and the overall number of vertex of these cluster be give to the clusterer as additional information intriguingly this additional information be not sufficient for class include figure with hole while some class be unclusterable due to they topological structure other be only computationally intractable oracle can be use to distinguish between both case the former can not be cluster use any oracle while the latter can be cluster use some oracle it be show that there be maximal oracle that allow clustering of all problem that can be cluster with the help of some oracle in particular e be maximal iff e t k e doi 101137 050629112 hypothesis space clusterer disjoint number of cluster clusterability	SIAM_J._Comput.	
1060860	sanjay_jain frank_stephan	mitotic class in inductive inference		SIAM_J._Comput.	
1088841	andris_ambainis sanjay_jain arun_sharma	ordinal mind change complexity of language identification	the approach of ordinal mind change complexity introduce by freivalds and smith use lrb notation for rrb constructive ordinal to bind the number of mind change make by a learning machine this approach provide a measure of the extent to which a learn machine have to keep revise its estimate of the number of mind change it will make before converge to a correct hypothesis for language in the class be learn recently this notion which also yield a measure for the difficulty of learn a class of language have be use to analyze the learnability of rich concept class the present paper further investigate the utility of ordinal mind change complexity it be show that for identification from both positive and negative datum and n 1 the ordinal mind change complexity of the class of language form by union of up to n 1 pattern language be only o notn lrb n rrb lrb where notn lrb n rrb be a notation for n be a notation for the least limit ordinal and o represent ordinal multiplication rrb this result nicely extend a observation of lange and zeugmann that pattern language can be identify from both positive and negative datum with 0 mind change existence of a ordinal mind change bind for a class of learnable language can be see as a indication of its learning tractability condition be investigate under which a class have a ordinal mind change bind for identification from positive datum it be show that a index family of language have a ordinal mind preprint submit to elsevier science change bind if it have finite elasticity and can be identify by a conservative machine it be also show that the requirement of conservative identification can be sacrifice for the purely topological requirement of mfinite thickness interaction between identification by monotonic strategy and existence of ordinal mind change bind be also investigate doi 101007 354062685925 pattern language txtex mind change ordinal finite thickness	Theor._Comput._Sci.	
1088921	hiroki_arimura sanjay_jain	preface		Theor._Comput._Sci.	
1089922	john_case keh-jiann_chen sanjay_jain	cost of general purpose learning	leo harrington surprisingly construct a machine which can learn any computable function f accord to the follow criterion lrb call bc identification rrb he machine on the successive graph point of f output a corresponding infinite sequence of program p 0 p each compute a variant of f which differ from f at only finitely many argument place a machine with this property be call general purpose the sequence for harrington s general purpose machine for distinct m and n the finitely many argument place where p i m fail to compute f can be very different from the finitely many argument place where p i n fail to compute f one would hope though that if harrington s machine or a improvement thereof infer the program p i m base on the datum point f lrb 0 rrb f lrb 1 rrb f lrb k rrb then p i m would make very few mistake compute f at the near future argument k 1 k 2 k where be reasonably large ideally p i m s finitely many mistake or anomaly would lrb mostly rrb occur at argument x k ie ideally its anomaly would be well place beyond near future argument in the present paper for general purpose learn machine it be analyze just how well or badly place these anomaly may be with respect to near future argument and what be the various tradeoff in particular there be good news and bad bad news be that for any learn machine m lrb include general purpose m rrb for all m there exist infinitely many computable function f such that infinitely often m incorrectly predict f s next m near future value good news be that for a suitably clever general purpose learn machine m for each computable f for m on f the density of any such associated bad prediction interval of size m be vanishingly small consider too be the possibility of provide a general purpose learner which additionally learn some interesting class with respect to much stricter criterion than bc identification again there be good news and bad the criterion of finite identification require for success that a learner m on a function f output exactly one program which correctly compute f bc nidentification be just like bc identification above except that the number of anomaly in each program of a final sequence be n bad news be that doi 101016 s03043975 lrb 00 rrb 000281 computable general purpose function f anomaly pro	Theor._Comput._Sci.	
1089923	john_case sanjay_jain susanne_kaufmann arun_sharma frank_stephan	predictive learning model for concept drift	concept drift mean that the concept about which data be obtain may shift from time to time each time after some minimum permanence except for this minimum permanence the concept shift may not have to satisfy any further requirement and may occur infinitely often within this work be study to what extent it be still possible to predict or learn value for a data sequence produce by drift concept various way to measure the quality of such prediction include martingale bet strategy and density and frequency of correctness be introduce and compare with one another for each of these measure of prediction quality for some interesting concrete class lrb nearly rrb optimal bound on permanence for attain learnability be establish the concrete class from which the drift concept be select include regular language accept by finite automata of bound size polynomial of bound degree and sequence define by recurrence relation of bound size some important restricted case of drift be also study for example the case where the interval of permanence be computable in the case where the concept shift only 1 among finitely many possibility from certain infinite arguably practical class the learn algorithm can be considerably improve doi 101007 354049730721 drift martingale permanence nse iff	Theor._Comput._Sci.	Univ._of_Singapore Singapore
1089924	john_case sanjay_jain frank_stephan	vacillatory and bc learning on noisy datum	in a earlier paper frank stephan introduce a form of noisy datum which nonetheless uniquely determine the true datum correct information occur innnitely often while incorrect information occur only nitely often the present paper consider the eeect of this form of noise on vacillatory and behaviorally correct learning of grammar both from positive datum alone and from informant lrb positive and negative datum rrb for learn from informant the noise in eeect destroy negative datum various noisydata hierarchy be exhibit which in some case be know to collapse when there be no noise noisy behaviorally correct learning be show to obey a very strong subset principle it be show in many case how much power be need to overcome the eeect of noise for example the best we can do to simulate in the presence of noise the noisefree no mind change case take innnitely many mind change one technical result be prove by a priority argument doi 101007 354061863553 eeect informant negative datum correct information	Theor._Comput._Sci.	National_Univ._of_Singapore Singapore
1089925	john_case sanjay_jain arun_sharma	synthesize noisetolerant language learner	a index for a re class of language lrb by deenition rrb generate a sequence of grammar deen the class a index for a index family of language lrb by deenition rrb generate a sequence of decision procedure deen the family f stephan s model of noisy datum be employ in which roughly correct datum crop up innnitely often and incorrect datum only nitely often study then be the synthesis from index for re class and for index family of language of various kind of noisetolerant languagelearner for the corresponding class or family index many positive result as well as some negative result be present regard the existence of such synthesizer the proof of most of the positive result yield as pleasant corollary strict subsetprinciple or telltale style characterization for the noisetolerant learnability of the corresponding class or family index doi 101016 s03043975 lrb 00 rrb 001328 deenition positive result re class	Theor._Comput._Sci.	National_Univ._of_Singapore Singapore
1089926	john_case sanjay_jain mandayam_suraj	control structure in hypothesis space the influence on learning	in any learnability setting hypothesis be conjecture from some hypothesis space study herein be the influence on learnability of the presence or absence of certain control structure in the hypothesis space first present be control structure characterization of some rather specific but illustrative learnability result the presence of these control structure be thereby show essential to maintain full learn power then present be the main theorem each of these nontrivially characterize the invariance of a learning class over hypothesis space v and the presence of a particular projection control structure call proj in v as v have suitable instance of all denotational control structure in a sense then proj epitomize the control structure whose presence need not help and whose absence need not hinder learn power doi 101016 s03043975 lrb 00 rrb 003856 hypothesis space learnability control structure decoration proj	Theor._Comput._Sci.	National_Univ._of_Signapore Singapore Republic_of_Singapore
1089927	john_case sanjay_jain arun_sharma	anomalous learning help succinctness	it be show that allow a bound number of anomaly lrb mistake rrb in the final program learn by a algorithmic procedure can considerably succinctify those final program naturally only those context be investigate in which the presence of anomaly be not actually require for successful inference lrb learning rrb the context consider be certain infinite subclass of the class of characteristic function of finite set for each finite set d these subclass have a finite set contain d this latter prevent the anomaly from wipe out all the information in the set feature in these subclass and show the context to be fairly robust some of the result in the present paper be show to be provably more constructive than other the result of this paper can also be interpret as fact about succinctness of code finite set which fact have interesting consequence for learnability of decision procedure for finite set doi 101016 03043975 lrb 95 rrb 001549 finite set corollary characteristic function anomaly algorithmic	Theor._Comput._Sci.	
1091264	rusins_freivalds sanjay_jain	kolmogorov numberings and minimal identification	identification of program for computable function from they graph by algorithmic device be a well study problem in learn theory freivalds and chen consider identification of minimal and nearly minimal program for function from they graph to address certain problem in minimal identification for gdel numbering freivalds later consider minimal identification in kolmogorov numbering kolmogorov numbering be in some sense optimal numbering and have some nice property we prove certain separation result for minimal identification in every kolmogorov numbering in addition we also compare minimal identification in gdel numbering versus minimal identification in kolmogorov numbering doi 101007 3540591192177 formal definition numbering class of function minimal identification criterion	Theor._Comput._Sci.	
1091310	mark_a._fulk sanjay_jain	learning in the presence of inaccurate information	the present paper consider the effect of introduce inaccuracy in a learner s environment in gold s learning model of identification in the limit three kind of inaccuracy be consider presence of spurious datum be model as learn from a noisy environment miss datum be model as learn from incomplete environment and the presence of a mixture of both spurious and missing datum be model as learn from imperfect environment two learn domain be consider namely identification of program from graph of computable function and identification of grammar from positive datum about recursively enumerable language many hierarchy and tradeoff result from the interplay between the number of error allow in the final hypothesis the number of inaccuracy in the datum the type of inaccuracy and the type of success criterion be derive a interesting result be that in the context of function learning incomplete datum be strictly worse for learn than noisy datum doi 101016 03043975 lrb 95 rrb 001352 learning machine imperfect inaccuracy incomplete ers	Theor._Comput._Sci.	
1092297	sanjay_jain	branch and bind on the network model	karp and zhang develop a general randomize parallel algorithm for solve branch and bind problem they show that with high probability they algorithm attain optimal speedup within a constant factor lrb for p n lrb log n rrb c where p be the number of processor n be the size of the problem and c be a constant rrb ranade later simplify the analysis and obtain a better processor bind karp and zhang s algorithm work on model of computation where communication cost be constant the present paper consider the branch and bind problem on network where the communication cost be high suppose send a message in a p processor network take g o lrb log p rrb time and node expansion lrb define below rrb take unit time lrb other operation be free rrb then a simple randomized algorithm be present which be asymptotically nearly optimal for p o lrb 2 log c n rrb where c be any constant 13 and n be the number of node in the input tree with cost no greater than the cost of the optimal leaf in the tree doi 101007 354060692037 number of processor communication cost optimization problem	Theor._Comput._Sci.	National_Univ._of_Singapore Singapore Singapore
1092298	sanjay_jain efim_b._kinber steffen_lange rolf_wiehagen thomas_zeugmann	learning language and function by erase	learning by erase mean the process of eliminate potential hypothesis from further consideration thereby converge to the least hypothesis never eliminate this hypothesis must be a solution to the actual learning problem the capability of learn by erase be investigate in relation to two factor the choice of the overall hypothesis space itself and what set of hypothesis must or may be erase these learn capability be study for two fundamental kind of object to be learn namely language and function for learn language by erase the case of learn index family be investigate a complete picture of all separation and coincidence of the consider model be derive learning by erase be compare with standard model of language learn such as learn in the limit nite learn and conservative learning the exact location of these type within the hierarchy of the model of learn by erase be establish necessary and sucient condition for language learning by erase be present for learn function by erase mainly the case of learn minimal program be study various relationship and dierence between the consider type of function learning by erase and also to standard function learning be exhibit in particular these type be explore in kolmogorov numbering that can be view as natural g odel numbering of the partial recursive function necessary and sucient condition for function learning by erase be derive doi 101016 s03043975 lrb 99 rrb 002698 language learning numbering function learning erase se	Theor._Comput._Sci.	National_Univ._of_Singapore Singapore
1092299	sanjay_jain efim_b._kinber rolf_wiehagen thomas_zeugmann	on learning of function refutably	learning of recursive function refutably informally mean that for every recursive function the learn machine have either to learn this function or to refute it that be to signal that it be not able to learn it three modi of make precise the notion of refute be consider we show that the corresponding type of learn refutably be of strictly increase power where already the most stringent of they turn out to be of remarkable topological and algorithmical richness furthermore all these type be close under union though in different strength also these type be show to be different with respect to they intrinsic complexity two of they do not contain function class that be most difficult to learn while the third one do moreover we present several characterization for these type of learn refutably some of these characterization make clear where the refute ability of the corresponding learning machine come from and how it can be realize in general for learn with anomaly refutably we show that several result from standard learning without refutation stand refutably from this we derive some hierarchy for refutable learning finally we prove that in general one can not trade stricter refutability constraint for more liberal learning criterion doi 101016 s03043975 lrb 02 rrb 004218 refuting learning machine refutation anomaly	Theor._Comput._Sci.	
1092300	sanjay_jain steffen_lange jochen_nessel	on the learnability of recursively enumerable language from good example	the present paper investigate identification of index family l of recursively enumerable language from good example we distinguish class preserving learn from good example lrb the good example have to be generate with respect to a hypothesis space have the same range as l rrb and class comprise learning from good example lrb the good example have to be select with respect to a hypothesis space comprise the range of l rrb a learner be require to learn a target language on every finite superset of the good example for it if the learner s first and only conjecture be correct then the underlie learning model be refer to as finite identification from good example and if the learner make a finite number of incorrect conjecture before always output a correct one the model be refer to as limit identification from good example in the context of class preserving learning it be show that the learn power of finite and limit identification from good text example coincide when class comprise learning from good text example be concern limit identification be strictly more powerful than finite learning furthermore if learn from good informant example be consider limit identification be superior to finite identification in the class preserving as well as in the class comprise case finally we relate the model of learn from good example to one another as well as to the standard learning model in the context of goldstyle language learning doi 101016 s03043975 lrb 00 rrb 001316 hypothesis space finite identification pupil finite number grammar	Theor._Comput._Sci.	National_Univ._of_Singapore Singapore
1092301	sanjay_jain arun_sharma	mind change complexity of learn logic program	the present paper motivate the study of mind change complexity for learn minimal model of lengthbounded logic program it establish ordinal mind change complexity bound for learnability of these class both from positive fact and from positive and negative fact building on angluin s notion of finite thickness and wright s work on finite elasticity shinohara define the property of bound finite thickness to give a sufficient condition for learnability of index family of computable language from positive datum this paper show that a effective version of shinohara s notion of bound finite thickness give sufficient condition for learnability with ordinal mind change bind both in the context of learnability from positive datum and for learnability from complete lrb both positive and negative rrb datum let be a notation for the first limit ordinal then it be show that if a language define framework yield a uniformly decidable family of language and have effective bound finite thickness then for each natural number m 0 the class of language define by formal system of length m be identifiable in the limit from positive datum with a mind change bind of m be identifiable in the limit from both positive and negative datum with a ordinal mind change bind of m the above sufficient condition be employ to give a ordinal mind change bind for learnability of minimal model of various class of lengthbounded prolog program include shapiro s linear program arimura and shinohara s depthbounded linearlycovering program and krishna rao s depthbounded linearlymoded program it be also note that the bind for learn from positive datum be tight for the example class consider machine learning in the context of firstorder logic and its subclass can be trace back to the work of plotkin lsb plo71 rsb and shapiro lsb sha81 rsb in recent year this work have evolve into the very active field of inductive logic programming lrb ilp rrb numerous practical system have be build to demonstrate the feasibility of learn logic program as description of complex concept the utility of these system have be demonstrate in many domain include drug design protein secondary structure prediction and finite element mesh design lrb see muggleton and deraedt lsb mdr94 rsb lavrac and dzeroski lsb ld94 rsb bergadano and gunetti lsb bg96 rsb and nienhuyscheng and de wolf lsb ncdw97 rsb for a survey of this field rrb together with practical development there have also be some interest in derive learnability theorem for ilp several result doi 101016 s03043975 lrb 01 rrb 000846 mind change learnability logic program positive datum ilp	Theor._Comput._Sci.	School_of_Computing National_University_of_Singapore Singapore_119260 Republic_of_Singapore
1092302	sanjay_jain frank_stephan	learning how to separate	braincomputer interface lrb bci rrb extract signal from neural activity to control remote device range from computer cursor to limblike robot they show great potential to help patient with severe motor deficit perform everyday task without the constant assistance of caregiver understand the neural mechanism by which subject use bci system could lead to improve design and provide unique insight into normal motor control and skill acquisition however report vary considerably about how much training be require to use a bci system the degree to which performance improve with practice and the underlie neural mechanism this review examine these diverse finding they potential relationship with motor learning during overt arm movement and other outstanding question concern the volitional control of bci system doi 101016 jtins 201011003 arm movement control signal neural mechanism bci bmi	Theor._Comput._Sci.	School_of_Computing National_University_of_Singapore Singapore_119260 Singapore
1092303	sanjay_jain arun_sharma	program size restriction in computational learning	a model for a subject s learn its environment e could be describe thus s place in e receive datum about e and simultaneously conjecture a sequence of hypothesis s be say to learn e just in case the sequence of hypothesis conjecture by s stabilize to a final hypothesis which correctly represent e computational learn theory provide a framework for study problem of this nature when the subject be a machine a natural abstraction for the notion of hypothesis be a computer program the present paper in the above framework of learning present argument for the final hypothesis to be succinct and introduce a plethora of formulation of such succinctness a revelation of this study be that some of the natural notion of succinctness may be uninteresting because learn capability of machine under these seemingly natural constraint be dependent on the choice of programming system use to interpret hypothesis doi 101016 03043975 lrb 94 rrb 900477 language acquisition computer program natural sequence of hypothesis succinctness	Theor._Comput._Sci.	
1092304	sanjay_jain arun_sharma	on aggregate team of learning machine	1 the present paper study the problem of when a team of learn machine can be aggregate into a single learning machine without any loss in learn power the main result concern aggregation ratio for vacillatory identiication of language from text for a positive i n teger n a m a c hine be say to txtfex nidentify a language l just in case the machine converge to up to n grammar for l on any text for l f or such identiication criterion the aggregation ratio be derive for the n 2 case it be show that the collection of language that can be txtfex 2 identii by team with success ratio greater than 5 6 be the same as those collection of language that can be txtfex 2identiied by a single machine it be also establish that 5 6 be indeed the cutoo point by show that there be collection of language that can be txtfex 2identiied by a team employ 6 machine at least 5 of which be require to be successful but can not be txtfex 2identiied by a n y single machine additionally aggregation ratio be also derive for nite identiication of language from positive datum and for numerous criterion involve language learn from both positive and negative datum doi 101016 03043975 lrb 94 rrb 00162c identiication txtex success ratio single machine grammar	Theor._Comput._Sci.	
1095557	john_case sanjay_jain rudiger_reischuk frank_stephan thomas_zeugmann	learn a subclass of regular pattern in polynomial time	present be a algorithm lrb for learn a subclass of erase regular pattern language rrb which can be make to run with arbitrarily high probability of success on extended regular language generate by pattern of the form x01x1 mxm for unknown m but known c from number of example polynomial in m lrb and exponential in c rrb where x0 xm be variable and where 1 m be each string of constant or terminal of length c this assume that the algorithm randomly draw sample with natural and plausible assumption on the distribution the more general look case of extended regular pattern which alternate between a variable and fixed length constant string begin and end with either a variable or a constant string be similarly handle doi 101016 jtcs 200607044 subclass regular pattern length c constant string	Theor._Comput._Sci.	School_of_Computing National_University_of_Singapore Singapore Singapore
1095815	sanjay_jain jochen_nessel frank_stephan	invertible class	foreword this technical report contain a research paper development or tutorial article which have be submit for publication in a journal or for consideration by the commission organization the report represent the idea of its author and should not be take as the official view of the school or the university any discussion of the content of the report should be send to the author at the address show on the cover abstract this paper consider when one can invert general recursive operator which map a class of function f to f in this regard we study four different notion of inversion we additionally consider enumeration of operator which cover all general recursive operator which map f to f in the sense that for every general recursive operator mapping f to f there be a general recursive operator in the enumerated sequence which behave the same way as on f three different possible type of enumeration be study	Theor._Comput._Sci.	
1095864	sanjay_jain efim_b._kinber	learning multiple language in group	we consider a variant of gold s learning paradigm where a learner receive as input n different language lrb in form of one text where all input language be interleave rrb we goal be to explore the situation when a more coarse classification of input language be possible whereas more refined classification be not more specifically we answer the follow question under which condition a learner be feed n different language can produce m grammar cover all input language but can not produce k grammar cover input language for any k m we also consider a variant of this task where each of the output grammar may not cover more than r input language we main result indicate that the major factor affect classification capability be the difference n m between the number n of input language and the number m of output grammar we also explore relationship between classification capability for smaller and larger group of input language for the variant of we model with the upper bind on the number of language allow to be represent by one output grammar for class consist of disjoint language we find complete picture of relationship between classification capability for different parameter n lrb the number of input language rrb m lrb number of output grammar rrb and r lrb bind on the number of language represent by each output grammar rrb this picture include a combinatorial characterization of classification capability for the parameter n m r of certain type doi 101016 jtcs 200707025 corollary input language n language line of research classification capability	Theor._Comput._Sci.	School_of_Computing National_University_of_Singapore Singapore_117590 Singapore
1095877	sanjay_jain steffen_lange sandra_zilles	a general comparison of language learn from example and from query	in language learning strong relationship between goldstyle model and query model have recently be observe in some quite general setting goldstyle learner can be replace by query learner and vice versa without loss of learn capability these equality hold in the context of learn indexable class of recursive language former study on goldstyle learning of such indexable class have show that in many setting the enumerability of the target class and the recursiveness of its language be crucial for learnability moreover study query learning nonindexable class have be mainly neglect up to now so it be conceivable that the recently observe relation between goldstyle and query learning be not due to common structure in the learn process in both model but rather to the enumerability of the target class or the recursiveness of they language in this paper the analysis be lift onto the context of learn arbitrary class of recursively enumerable language still strong relationship between the approach of goldstyle and query learning be prove but there be significant change to the former result though in many case learner of one type can still be replace by learner of the other type in general this do not remain valid vice versa all result hold even for learn class of recursive language which indicate that the recursiveness of the language be not crucial for the former equality result thus we analyze how constraint on the algorithmic structure of the target class affect the relation between two approach to language learning doi 101016 jtcs 200707024 language learning target concept recursiveness query learning strong relationship	Theor._Comput._Sci.	School_of_Computing National_University_of_Singapore Singapore_117590 Singapore
1095922	sanjay_jain efim_b._kinber	learning language from positive datum and a limited number of short counterexample	we consider two variant of a model for learn language in the limit from positive datum and a limited number of short negative counterexample lrb counterexample be consider to be short if they be smaller than the largest element of input see so far rrb negative counterexample to a conjecture be example which belong to the conjectured language but do not belong to the input language within this framework we explore howwhen learner use n short lrb arbitrary rrb negative counterexample can be simulated lrb or simulate rrb use least short counterexample or just no answer from a teacher we also study how a limited number of short counterexample fair against unconstrained counterexample and also compare they capability with the datum that can be obtain from subset superset and equivalence query lrb possibly with counterexample rrb a surprising result be that just one short counterexample can sometimes be more useful than any bounded number of counterexample of arbitrary size most of result exhibit salient example of language learnable or not learnable within corresponding variant of we model doi 101016 jtcs 200708010 arbitrary gnc bounded number short counterexamples positive datum	Theor._Comput._Sci.	School_of_Computing National_University_of_Singapore Singapore_117590 Singapore
1096072	sanjay_jain efim_b._kinber	learning and extend sublanguage	a number of natural model for learn in the limit be introduce to deal with the situation when a learner be require to provide a grammar cover the input even if only a part of the target language be available example of language family be exhibit that be learnable in one model and not learnable in another one some characterization for learnability of algorithmically enumerable family of language for the model in question be obtain since learnability of any part of the target language do not imply monotonicity of the learning process we consider also we model under additional monotonicity constraint doi 101007 1189484114 learnability sublanguage monotonicity target language grammar	Theor._Comput._Sci.	School_of_Computing National_University_of_Singapore 3_Science_Drive_2 Singapore_117543 Singapore
1096079	sanjay_jain eric_martin frank_stephan	absolute versus probabilistic classification in a logical setting	suppose we be give a set w of logical structure or possible world a set of logical formula call possible datum and a logical formula we then consider the classification problem of determine in the limit and almost always correctly whether a possible world m satisfy from a complete enumeration of the possible datum that be true in m one interpretation of almost always correctly be that the classification might be wrong on a set of possible world of measure 0 with respect to some natural probability distribution over the set of possible world another interpretation be that the classifier be only require to classify a set w of possible world of measure 1 without have to produce any claim in the limit on the truth of for the member of the complement of w in w we compare these notion with absolute classification of w with respect to a formula that be almost always equivalent to in w hence investigate whether the set of possible world on which the classification be correct be definable we mainly work with the probability distribution that correspond to the standard measure on the cantor space but we also consider a alternative probability distribution propose by solomonoff and contrast it with the former finally in the spirit of the kind of computation consider in logic programming we address the issue of compute almost correctly in the limit witness to lead existentially quantify variable in existential formula doi 101007 1156408926 logical formula successor possible world measure 1 possible datum	Theor._Comput._Sci.	School_of_Computing National_University_of_Singapore Singapore_117590 Republic_of_Singapore
1096552	sanjay_jain frank_stephan ye_nan	prescribe learning of re class	this work extend study of angluin lange and zeugmann on the dependence of learn on the hypothesis space choose for the class in subsequent investigation uniformly recursively enumerable hypothesis space have be consider in the present work the follow four type of learn be distinguish classcomprising lrb where the learner can choose a uniformly recursively enumerable superclass as hypothesis space rrb classpreserving lrb where the learner have to choose a uniformly recursively enumerable hypothesis space of the same class rrb prescribe lrb where there must be a learner for every uniformly recursively enumerable hypothesis space of the same class rrb and uniform lrb like prescribe but the learner have to be synthesize effectively from a index of the hypothesis space rrb while for explanatory learning these four type of learnability coincide some or all be different for other learn criterion for example for conservative learning all four type be different several result be obtain for vacillatory and behaviourally correct learning three of the four type can be separate however the relation between prescribed and uniform learning remain open it be also show that every lrb not necessarily uniformly recursively enumerable rrb behaviourally correct learnable class have a prudent learner that be a learner use a hypothesis space such that it learn every set in the hypothesis space moreover the prudent learner can be effectively build from any learner for the class doi 101016 jtcs 200901011 uniform learnability hypotheses space iff same class	Theor._Comput._Sci.	
1227594	sanjay_jain efim_b._kinber	oneshot learner use negative counterexample and nearest positive example	as some cognitive research suggest in the process of learn language in addition to overt explicit negative evidence a child often receive covert explicit evidence in form of correct or rephrase sentence in this paper we suggest one approach to formalization of overt and covert evidence within the framework of oneshot learner via subset and membership query to a teacher lrb oracle rrb we compare and explore general capability of we model as well as complexity advantage of learnability model of one type over model of other type where complexity be measure in term of number of query in particular we establish that correct positive example be sometimes more helpful to a learner than just negative lrb counter rrb example and access to full positive datum doi 101007 978354075225722 membership query finite positive example number of query counterexample	Theor._Comput._Sci.	School_of_Computing National_University_of_Singapore Singapore_117417 Singapore
1237536	sanjay_jain qinglong_luo pavel_semukhin frank_stephan	uncountable automatic class and learning	in this paper we consider uncountable class recognizable by automata and investigate suitable learning paradigm for they in particular the counterpart of explanatory vacillatory and behaviourally correct learning be introduce for this setting here the learner read in parallel the datum of a text for a language l from the class plus a index and output a sequence of automata such that all but finitely many of these automata accept the index iff be a index for l it be show that any class be behaviourally correct learnable if and only if it satisfy angluin s telltale condition for explanatory learning such a result need that a suitable indexing of the class be choose on the one hand every class satisfying angluin s telltale condition be vacillatory learnable in every indexing on the other hand there be a fixed class such that the level of the class in the hierarchy of vacillatory learning depend on the indexing of the class choose we also consider a notion of blind learning on the one hand a class be blind explanatory lrb vacillatory rrb learnable if and only if it satisfy angluin s telltale condition and be countable on the other hand for behaviourally correct learning there be no difference between the blind and nonblind version this work establish a bridge between automata theory and inductive inference lrb learning theory rrb doi 101016 jtcs 201012057 countable class indexing automata learnability natural number	ALT	
1237540	sanjay_jain efim_b._kinber	iterative learning from texts and counterexamples use additional information	a variant of iterative learning in the limit lrb cf lsb lz96 rsb rrb be study when a learner get negative example refute conjecture contain datum in excess of the target language and use additional information of the follow four type a rrb memorize up to n input element see so far b rrb up to n feedback membership query lrb testing if a item be a member of the input see so far rrb c rrb the number of input element see so far d rrb the maximal element of the input see so far we explore how additional information available to such learner lrb define and study in lsb jk07 rsb rrb may help in particular we show that add the maximal element or the number of element see so far help such learner to infer any index class of language classpreservingly lrb use a descriptive numbering define the class rrb as it be prove in lsb jk07 rsb this be not possible without use additional information we also study how in the give context different type of additional information fare against each other and establish hierarchy of learner memorize n 1 versus n input element see and n 1 versus n feedback membership query doi 101007 s1099401152387 number of element longterm memory learnability maximal element counterexample	ALT	
1237546	sanjay_jain frank_stephan nan_ye	learning from streams	recent year have witness a incredibly increase interest in the topic of incremental learning unlike conventional machine learn situation datum flow target by incremental learning become available continuously over time accordingly it be desirable to be able to abandon the traditional assumption of the availability of representative training datum during the training period to develop decision boundary under scenario of continuous datum flow the challenge be how to transform the vast amount of stream raw datum into information and knowledge representation and accumulate experience over time to support future decisionmaking process in this paper we propose a general adaptive incremental learning framework name adain that be capable of learn from continuous raw datum accumulate experience over time and use such knowledge to improve future learning and prediction performance detailed system level architecture and design strategy be present in this paper simulation result over several realworld datum set be use to validate the effectiveness of this method doi 101109 tnn 20112171713	ALT	
1239195	sanjay_jain frank_stephan jason_teutsch	index sets and universal numberings	this paper study the turing degree of various property define for universal numbering that be for numbering which list all partialrecursive function in particular property relate to the domain of the corresponding function be investigate like the set deq of all pair of index of function with the same domain the set dmin of all minimal index of set and dmin of all index which be minimal with respect to equality of the domain modulo finitely many difference a partial solution to a question of schaefer be obtain by show that for every universal numbering with the kolmogorov property the set dmin be ture equivalent to the double jump of the halt problem furthermore it be show that the join of deq and the halt problem be ture equivalent to the jump of the halt problem and that there be numbering for which deq itself have 1generic turing degree doi 101007 978364203073428 numbering dmin deq recursive function halt	CiE	Department_of_Computer_Science National_University_of_Singapore Singapore Republic_of_Singapore_117417
1276130	sanjay_jain	on some open problem in monotonic and conservative learning	in this paper we solve some open problem in monotonic and conservative learning doi 101016 jipl 200904020 hypothesis space monotonic conservative open problem informant	Inf._Process._Lett.	School_of_Computing National_University_of_Singapore Singapore_117417
1280724	sanjay_jain charles_r._mclean	components of a incident management simulation and gaming framework and related development		Simulation	Department_of_Decision_Sciences_School_of_Business_GeorgeWashington_University_Washington DC_20052 USA
1340782	sanjay_jain eric_martin frank_stephan	inputdependence in functionlearning	in the standard model of inductive inference a learner get as input the graph of a function and have to discover lrb in the limit rrb a program for the function in this paper we consider besides the graph also other mode of input such as the complement of the graph the undergraph and the overgraph of the function the relationship between these model be study and a complete picture be obtain furthermore these notion be also explore for learn with oracle learn in team and learn in the presence of additional information doi 101007 s002240099174x finite set universe bridge positive datum oracle	Theory_Comput._Syst.	National_University_of_Singapore Department_of_Computer_Science 117417 Singapore Republic_of_Singapore
1347884	wilfred_amaldoss sanjay_jain	joint bidding in the nameyourownprice channel a strategic analysis		Management_Science	
1397352	sanjay_jain arun_sharma	elementary formal system intrinsic complexity and procrastination	recently rich subclass of elementary formal system lrb ef rrb have be show to be identifiable in the limit from only positive datum example of these class be angluin s pattern language union of pattern language by wright and shinohara and class of language definable by lengthbounded elementary formal system study by shinohara the present paper employ two distinct body of abstract study in the inductive inference literature to analyze the learnability of the these concrete class the first approach introduce by freivalds and smith use constructive ordinal to bind the number of mind change w denote the first limit ordinal a ordinal mind change bind of u mean that identification can be carry out by a learner that after examine some element lrb s rrb of the language announce a upper bind on the number of mind change it will make before converge a bind of 2w mean that the learner reserve the right to revise this upper bind once a bind of 3w mean the learner reserve the right to revise this upper bind twice and so on a bind of w2 mean that identification can be carry out by a learner that announce a upper bind on the number of time it may revise its conjectured upper bind on the number of mind change it be show in the present paper that the ordinal mind change complexity for identification of language form by union of up to n pattern language be wn it be also show that this bind be essential similar result be also permission to make digitalhard copy of all or part of this material for personal or classroom use be grant without fee provide that rhe copy be not make or distribute for profit or commercial advantage the copyright ootice the title of the publication and its date appear and notice ia show to hold for class definable by lengthbounded elementary formal system with up to n clause the second approach study by freivalds kinber and wiehagen and by jain and sharma employ reduction to study the intrinsic complexity of learnable class it be show that the class of language form by take union of up to n 1 pattern language be a strictly more difficult learning problem than the class of language form by the union of up to n pattern language it be also show that a similar hierarchy hold for the bind on the number of doi 101145 238061238093 pattern language mind change txtex	Information_and_Computation	
1550187	vivek_s._borkar sanjay_jain govindan_rangarajan	evolutionary game with two timescale	game theory be one of the key paradigm behind many scientific discipline from biology to behavioral science to economics in its evolutionary form and especially when the interact agent be link in a specific social network the underlie solution concept and method be very similar to those apply in nonequilibrium statistical physics this review give a tutorialtype overview of the field for physicist the first three section introduce the necessary background in classical and evolutionary game theory from the basic definition to the most important result the fourth section survey the topological complication imply by nonmeanfieldtype social network structure in general the last three section discuss in detail the dynamic behavior of three prominent class of model the prisoner s dilemma the rockscissorspaper game and compete association the major theme of the review be in what sense and how the graph structure of interaction can modify and enrich the picture of long term behavioral pattern emerge in evolutionary game doi 101016 jphysrep 200704004	Physica_D	
1560270	wilfred_amaldoss sanjay_jain	david vs goliath a analysis of asymmetric mixedstrategy games and experimental evidence	m ixe strategy be widely use to model strategic situation in diverse field such as economics marketing political science and biology however some of the implication of asymmetric mixedstrategy solution be counterintuitive we develop a stylized model of patent race to examine some of these implication in we model two firm compete to develop a product and obtain a patent however one firm value the patent more because of its market advantage such as brand reputation and distribution network contrary to some intuition we find that the firm that value the patent less be likely to invest more aggressively in develop the product and will also win the patent more often we argue that the reason for these counterintuitive result be inherent in the very concept of mixed strategy solution in a laboratory test we examine whether subject behavior conform to the equilibrium prediction we find that the aggregate behavior of we subject be consistent with the gametheoretic prediction with the help of the experienceweighted attraction lrb ewa rrb learning model propose by camerer and ho lrb 1999 rrb we show that adaptive learning can account for the investment behavior of we subject we find that the ewa learning model track the investment decision of we subject well whether we hold out trial or a entire group of subject doi 101287 mnsc 488972165 distribution network bid patent symmetric game mixed strategy	Management_Science	
1565714	sanjay_jain p._k._kannan	pricing of information products on online servers issue model and analysis	o nline information server that provide access to diverse database where user can search for browse through and download the information they need have be rapidly increase in number in the past few year online vendor have traditionally charge user for information on the base on the length of the time they be connect to the database with hardware and software advance many online server have recently start change they pricing strategy to searchbased andor subscriptionfee pricing this paper examine the various issue involve in price these information product and present a economic approach to analyze condition under which the various pricing scheme may prove optimal for the online server we result show that the variation in consumer expertise and valuation of information affect the choice of a pricing strategy by the server we present general condition under which subscriptionfee pricing be optimal even when consumer demand be inelastic we also find that give the cost structure characterize the market undifferentiated online server can compete and coexist in the market each make positive profit we show that in a competitive set a increase in cost of online server can sometimes benefit they by enable they to differentiate themselves we result offer insight into the trend in pricing strategy and may provide a explanation as to why many server may persist with connecttime strategy doi 101287 mnsc 4891123178 pricing information product online server information good connecttime	Management_Science	
1596224	sanjay_jain frank_stephan nan_ye	prescribe learning of re class	this work extend study of angluin lange and zeugmann on the dependence of learn on the hypothesis space choose for the class in subsequent investigation uniformly recursively enumerable hypothesis space have be consider in the present work the follow four type of learn be distinguish classcomprising lrb where the learner can choose a uniformly recursively enumerable superclass as hypothesis space rrb classpreserving lrb where the learner have to choose a uniformly recursively enumerable hypothesis space of the same class rrb prescribe lrb where there must be a learner for every uniformly recursively enumerable hypothesis space of the same class rrb and uniform lrb like prescribe but the learner have to be synthesize effectively from a index of the hypothesis space rrb while for explanatory learning these four type of learnability coincide some or all be different for other learn criterion for example for conservative learning all four type be different several result be obtain for vacillatory and behaviourally correct learning three of the four type can be separate however the relation between prescribed and uniform learning remain open it be also show that every lrb not necessarily uniformly recursively enumerable rrb behaviourally correct learnable class have a prudent learner that be a learner use a hypothesis space such that it learn every set in the hypothesis space moreover the prudent learner can be effectively build from any learner for the class doi 101016 jtcs 200901011 uniform learnability hypotheses space iff same class	Proceedings_of_the_18th_international_conference_on_Algorithmic_Learning_Theory	Department_of_Computer_Science National_University_of_Singapore Singapore_117590 Republic_of Singapore
1661807	sanjay_jain	learning in the presence of additional information and inaccurate information	online learning algorithm often have to operate in the presence of concept drift lrb ie the concept to be learn can change with time rrb this paper present a new categorization for concept drift separate drift accord to different criterion into mutually exclusive and nonheterogeneous category moreover although ensemble of learn machine have be use to learn in the presence of concept drift there have be no deep study of why they can be helpful for that and which of they feature can contribute or not for that as diversity be one of these feature we present a diversity analysis in the presence of different type of drift we show that before the drift ensemble with less diversity obtain lower test error on the other hand it be a good strategy to maintain highly diverse ensemble to obtain lower test error shortly after the drift independent on the type of drift even though high diversity be more important for more severe drift longer after the drift high diversity become less important diversity by itself can help to reduce the initial increase in error cause by a drift but do not provide the faster recovery from drift in longterm doi 101109 tkde 2009156 categorization ustc concept drift professor test error	null	
1711503	sanjay_jain arun_sharma mahendran_velauthapillai	finite identification of function by team with success ratio 12 and above	consider a scenario in which a algorithmic machine m be be feed the graph of a computable function f m be say to finitely identify f just in case after inspect a finite portion of the graph of f it emit its first conjecture which be a program for f and it never abandon this conjecture thereafter a team of machine be a multiset of such machine a team be say to be successful just in case each member of some nonempty subset of predetermined size of the team be successful the ratio of the number of machine require to be successful to the size of the team be refer to as the success ratio of the team the present paper investigate the finite identification of computable function by team of learn machine the result present complete the picture for team with success ratio 1 2 and greater it be show that at success ratio 1 2 introduce redundancy in the team can result in increase learn power in particular it be establish that larger collection of function can be learn by employ team of 4 machine and require at least 2 to be successful than by employ team of 2 machine and require at least 1 to be successful surprisingly it be also show that introduce further redundancy at success ratio 1 2 do not yield any extra learning power in particular it be show that the collection of function that can be finitely identify by a team of 2m machine require at least m to be successful be the same as the collection of function that can be finitely identify by a team of 4 machine require at least 2 to be successful if m be even and the collection of function that can be identify by a team of 2 machine require at least 1 to be successful if m be odd these latter result require development of sophisticated simulation technique doi 101006 inco 19951133	Information_and_Computation	
1719033	william_j._foley sanjay_jain jorge_haddock	use simulation generator for modeling flexible manufacturing system	this introductory tutorial be a overview of simulation modeling and analysis many critical question be answer in the paper what be modeling what be simulation what be simulation modeling and analysis what type of problem be suitable for simulation how to select simulation software what be the benefit and pitfall in modeling and simulation the intended audience be those unfamiliar with the area of discrete event simulation as well as beginner look for a overview of the area this include anyone who be involve in system design and modificationsystem analyst management personnel engineer military planner economist banking analyst and computer scientist familiarity with probability and statistics be assume modeling be the process of produce a model a model be a representation of the construction and working of some system of interest a model be similar to but simpler than the system it represent one purpose of a model be to enable the analyst to predict the effect of change to the system on the one hand a model should be a close approximation to the real system and incorporate most of its salient feature on the other hand it should not be so complex that it be impossible to understand and experiment with it a good model be a judicious tradeoff between realism and simplicity simulation practitioner recommend increase the complexity of a model iteratively a important issue in modeling be model validity model validation technique include simulate the model under known input condition and compare model output with system output generally a model intend for a simulation study be a mathematical model develop with the help of simulation software mathematical model classification include deterministic lrb input and output variable be fix value rrb or stochastic lrb at least one of the input or output variable be probabilistic rrb static lrb time be not take into account rrb or dynamic lrb timevarying interaction among variable be take into account rrb typically simulation model be stochastic and dynamic a simulation of a system be the operation of a model of the system the model can be reconfigure and experiment with usually this be impossible too expensive or impractical to do in the system it represent the operation of the model can be study and hence property concern the behavior of the actual system or its subsystem can be infer in its broadest sense simulation be a tool to evaluate the performance of a system exist or propose under different doi 101145 268437268440 simulation modeling realism real actual analyst simulation software simulation study	Progress_in_simulation_(vol._2)	
